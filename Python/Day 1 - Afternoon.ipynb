{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0193d1c8-a4aa-4710-9bd1-c71434d9e8bd",
   "metadata": {},
   "source": [
    "# Day 1 - Afternoon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d3240f-3431-40e6-be70-c01cd149a519",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "<img src=\"./img/house-prices.png\" width=\"50%\">\n",
    "\n",
    "*Image generated using ChatGPT 4*\n",
    "\n",
    "So the topic of this course is of course Machine Learning, and more spefically practical or applied Machine Learning. So now we will discuss in general what Machine Learning actually is.\n",
    "\n",
    "So what is machine learning? One definition states:\n",
    "\n",
    "> *Machine learning is the field of study that gives computers the ability to learn without being explicitly programmed.*\n",
    "\n",
    "*Arthur Samuel, 1959*\n",
    "\n",
    "Arthur Samuel wrote a very early programme to learn to play the game of Checkers. The algorithm was designed to make random moves, and slowly learn which moves led to an advantage and which led to a disadvantage. It was designed to play against itself, and did so many 10s of thousands of times, eventually learning what good moves were and what bad moves were and to avoid them, and managed to learn to play Checkers to a top level: without being programmed to do this. This is a special type of Machine Learning method called Reinforcement Learning, and was one of the very first Machine Learning systems. It is used in many fields still today, including the system that beat the Go champion, AlphaGo. The basic premise, however, is that a computer can be made to learns from some data.\n",
    "\n",
    "More formally however, you can think of a machine learning algorithm as something whereby a system takes an input, $X$ which is our data, and produces an output $y$, which is our prediction. In other words, it learns a mapping fron input $X$, our data, to output $y$, our prediction.\n",
    "\n",
    "In **supervised learning**, we give our algorithm examples of input and output pairs: ($X$, $y$) pairs. Therefore, the algorithm will see input data and the correct 'answer' for this input data, and given enough of these pairs, should learn the mapping from input $X$ it output $y$. It should eventually learn that when given a new sample $X$, then it will give a very good prediction for $y$.\n",
    "\n",
    "The algorithm learns from these input/label pairs ($X$, $y$) and eventually will be able to predict the output based on some new input where you have no labelled data. This is known as **supervised machine learning**, as you provide the algorithm with your data and the correct answers. We will primarily be dealing with supervised machine learning in this course, as it is by far the most used type of machine learning.\n",
    "\n",
    "## Sub-Categories of Machine Learning\n",
    "\n",
    "### Supervised vs. Unsupervised Learning\n",
    "\n",
    "The first thing to note, is that machine learning is broken down in to a few sub-categories. The first major distinctions is the distinction between **supervised machine learning** and **unsupervised machine learning**. In supervised machine learning we train on data where we have examples of our data with the corresponding answers (or **labels**, as they are called), as we just discussed. Unsupervised learning deals with the training of algorithms where we have inout data, but we **do not have any labels or answers** for this data. \n",
    "\n",
    "In this course, we are going to be dealing exclusively with supervised learning, as it by far the most common type of machine learning problem.\n",
    "\n",
    "### Classification and Regression\n",
    "\n",
    "Within the sub-category of supervised learning, there are also two major branches: regression and classification. \n",
    "\n",
    "Regression deals with predicting some continuous value. For example a price of a house, or a temperature.  \n",
    "\n",
    "Classification deals with predicting a category like spam/not spam in the case of a spam filter, or malignant/benign in some medical application. There are other subcategories of classification, such as binary classification, where there are two possible prediction categories, or multi-class classification, where there can be many different prediction categories. Tomorrow we will train a neural network to predict an image class from a possible 1,000 classes.\n",
    "\n",
    "The distinction is important for choosing the method you wish to use to train a prediction algorithm. Not all algorithms can do both regression and classification for example, but many can. For example neural networks can be trained as regression algorithms or classification algorithms. $k$-Nearest Neighbors ($k$-NN), however, is a algorithm for classification that assigns a label to an input data point based on the majority class of its $k$ nearest neighbors, and cannot be used for regression problems, by virtue of its design.\n",
    "\n",
    "## Examples of Machine Learning Applications\n",
    "\n",
    "Let's just think about what are possible machine learning algorithms (remember we are focussing on supervised learning): \n",
    "\n",
    "| Input ($X$)         | Output ($y$)              | Application         |\n",
    "|---------------------|---------------------------|---------------------|\n",
    "| email               | spam (0/1)                | Spam filter         |\n",
    "| hand written digits | label (0, 1, ..., 9)      | Image recognition   |\n",
    "| english             | german                    | Machine translation |\n",
    "| house details       | house price (numerical)   | Regression          |\n",
    "\n",
    "Notes:\n",
    "\n",
    "- In the case of the email/spam example: this is known as a binary classification problem\n",
    "- In the case of the hand written image example: this is a multi-class classification problem\n",
    "- In the case of the house details: this is a regression problem, the output is a real number, the price of the house\n",
    "\n",
    "In all of these examples, in a supervised learning setting, you will train your algorithm with input $X$ and the correct answers, $y$. For example, for the spam filter, you will want to supply it with examples of emails and whether or not it is a spam email or not. With enough of these examples, the algorithm will begin to learn what types of emails are spam and which are genuine.\n",
    "\n",
    "Once a machine learning algorithm has been trained for spam filtering, you can then supply it with a new email (one it has never seen before), and it will try to predict the label (spam/not spam) based on this email's contents.\n",
    "\n",
    "We should also note at this point, is that applied machine learning is very much an engineering discipline. In other words, given a particular dataset, with a stated goal, there is a best pratice set of decisions that can be made about how to tackle the problem. And there are a lot of decisions that need to be made. Do we select logistic regression or a support vector machine. Do we need more data. Do we need to train longer. Are we *over-fitting* or are we *under-fitting*. All these decisions can be made in a systematic way based on the situation. The goal of this course is to remove as much of the guesswork as possible, and for you to be able to make informed decisions about how to tackle a particular machine learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7304c94-6fa4-478b-801f-34b281a04bbd",
   "metadata": {},
   "source": [
    "# Key Terminology\n",
    "\n",
    "Before we go any further, let's cover some basic terminology and we will also cover some conventions we will use for writing our code. \n",
    "\n",
    "Let us define the following terms:\n",
    "\n",
    "- **Features**: these are properties about the sample in question. In a house dataset, this would be square metres, does it have a pool, neighbourhood, etc.\n",
    "- **Labels** or **targets** or **classes**: this is what you are trying to predict. They are also what you need to supply an algorithm in a supervised setting, during training. In the house price dataset, this is the price of the house.\n",
    "- **Training data**: what we use to train our algorithm.\n",
    "- **Validation data** and **Test data**: what we use to validate and test our algorithm. These are normally a subset of your total data. Generally we \n",
    "\n",
    "By convention your training data is stored in `X`, uppercase and your label data is stored in `y`, lowercase. \n",
    "\n",
    "Also, if you are splitting our data into training and test sets, you will see the following `X_train`, `X_test` will contain our training data, and `y_train`, and `y_test` will contain our corresponding labels. \n",
    "\n",
    "We will explain later exactly what the purpose of the training and test set data are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3481c1-15b6-48e0-a99e-686c9a65c702",
   "metadata": {},
   "source": [
    "# Machine Learning Lifecycle\n",
    "\n",
    "Before we talk about any algorthimths. Let's first discuss a typical machine learning lifecyle.\n",
    "\n",
    "After this we will discuss Sci-Kit Learn, which is Python's most used Machine Learning library. Sci-Kit Learn is a collection of many different algorithms, all with a common API. If you can use one algorithm in SciKit Learn, you can pretty much use them all.\n",
    "\n",
    "## 1. Problem Definition\n",
    "- Define your objectives and success criteria\n",
    "- Frame the problem (classification, regression, clustering, is it a supervised problem or an upersupervised problem)\n",
    "\n",
    "## 2. Data Collection\n",
    "- Gather your data: how will you get it? Will it be delivered to you? In what form?\n",
    "- Consider how much data you have, and the quality of the data, as this mau determine what algorithms you can use\n",
    "\n",
    "## 3. Data Exploration & Analysis\n",
    "- Perform what is known as exploratory data analysis (EDA)\n",
    "- Visualize the distributions of the data (e.g. consider the distribution of your targets classes, e.g. malignant/benign in your dataset, for example)\n",
    "- Identify outliers and missing values, which you may need to correct in the next phase\n",
    "\n",
    "## 4. Data Preprocessing\n",
    "- Handle missing values\n",
    "- Remove or correct outliers\n",
    "- Normalize/standardize features\n",
    "- Encode categorical variables\n",
    "- Create new features (feature engineering) by combining features\n",
    "- Split data into train/validation/test sets\n",
    "\n",
    "## 5. Model Selection & Training\n",
    "- Select appropriate algorithms based on problem type (classificaton, regression, supervised, unsupervised)\n",
    "- Train your first models, naively\n",
    "- Perform hyperparameter tuning\n",
    "- Perhaps train more complex models\n",
    "\n",
    "## 6. Model Evaluation\n",
    "- Carefully evaluate models on validation data and test data\n",
    "- Avoid pitfalls when evaluating your models\n",
    "- Does your model meet the requirements set out in step 1?\n",
    "\n",
    "## 7. Model Deployment\n",
    "- This is the final step\n",
    "- Serialize and package the model in to a single file\n",
    "- Implement API endpoints or create some user interface to your model (how should people use it?)\n",
    "- We will discuss this step in much more detail tomorrow\n",
    "\n",
    "We will try to cover as many aspects of this pipeline as possible in this session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee526f8-0be3-4759-a86a-553c53934014",
   "metadata": {},
   "source": [
    "# Sci-Kit Learn\n",
    "\n",
    "Now we will move on to the Sci-Kit Learn machine learning library.\n",
    "\n",
    "If you are doing Machine Learning with Python, you will most likely eventually end up using Sci-Kit Learn for many of your machine learning tasks. \n",
    "\n",
    "Sci-Kit Learn is a library of machine learning tools, algorithms, and utilities, that covers classification, regression, and clustering (unsupervised learning). \n",
    "\n",
    "Because there are many algorithms that are implemented in SciKit Learn, we will cover the general usage of the package, and show a few examples of some of the algorimths. Most algorithms follow the same API conventions, so if you know how to train a few of the algorithms using Sci-Kit Learn, you will probably quite easily be able to use all of them without too much difficulty.\n",
    "\n",
    "If we just take a look at the amount of algorithms covered by Sci-Kit Learn it is overwhleming:\n",
    "\n",
    "<https://scikit-learn.org/stable/user_guide.html>\n",
    "\n",
    "Covering all these different methods would be overwhelming and we would not have enough time to ever cover a fraction of them here. Instead I will talk about how use SciKit Learn **in general**. \n",
    "\n",
    "Most algorithms use a very similar API. In other words, if you have your data prepared in a way that one of these algorithms will accept it, then you can probably apply quite a few algorithms very easily. \n",
    "\n",
    "We mentioned earlier that common convention is that your data is contained in a data structure called `X` and your labels are contained in a data structure called `y`.\n",
    "\n",
    "## Core Components\n",
    "\n",
    "**Estimators**:\n",
    "- All supervised models (classifiers, regressors) are estimators\n",
    "- The fundamental training API consists of the `fit()` and `predict()` methods\n",
    "- Convention:\n",
    "    - `model.fit(X, y)` is used for training your algorithm\n",
    "    - `model.predict(X)` is then used after training for prediction\n",
    "\n",
    "**Metrics**:\n",
    "- Classification: `accuracy_score()`, `precision_recall_fscore_support()`, `confusion_matrix()`, and most often used `classification_report()`\n",
    "- Regression: `mean_squared_error()`, `r2_score()`\n",
    "\n",
    "**Preprocessing**:\n",
    "- `StandardScaler`, `MinMaxScaler`: For feature scaling\n",
    "- `OneHotEncoder`, `LabelEncoder`: For categorical features\n",
    "- `Imputer`: For handling missing values\n",
    "\n",
    "**Model Selection**:\n",
    "- `train_test_split()`: Splits data into training and testing sets\n",
    "- `GridSearchCV()`: For hyperparameter tuning\n",
    "- `cross_val_score()`: For cross-validation evaluation\n",
    "\n",
    "**Models**: Organized by learning task\n",
    "- Classification: `LogisticRegression`, `RandomForestClassifier`, `SVC`, etc.\n",
    "- Regression: `LinearRegression`, `RandomForestRegressor`, etc.\n",
    "- Clustering: `KMeans`, `DBSCAN`, etc.\n",
    "- Dimensionality Reduction: `PCA`, `TSNE`, etc.\n",
    "\n",
    "## Consistent API Pattern\n",
    "\n",
    "To summarise, we can say that SciKit-Learn's functionality follows a consistent pattern throughout most of its built in algorithms.\n",
    "\n",
    "Namely, most SciKit-Learn pipelines for a machine learning \n",
    "\n",
    "1. Initialize the estimator with some set of parameters\n",
    "2. Prepare data using methods such as `train_test_split()`\n",
    "3. Fit to training data (using `fit(X, y)`)\n",
    "4. Predict on new data (using `predict(X)`) - new data generally being your test set.\n",
    "5. Evaluate results with built-in metrics tools, such as `classification_report()` and `confusion_matrix()`\n",
    "\n",
    "This consistent interface makes it easy to swap different algorithms while maintaining the same workflow structure.\n",
    "\n",
    "Sci-Kit Learn's API is used so often, that other packages mimick it so that they can be used interchangebly with SciKit-Learn's other functions. For example, a well known Random Forest algorithm called XGBoost also follows the identical API. \n",
    "\n",
    "## Core Algorithms \n",
    "\n",
    "### Linear Models\n",
    "- **Linear Regression**: Fits a linear relationship between features and target variables\n",
    "- **Ridge & Lasso Regression**: Linear regression with L2 and L1 regularization respectively\n",
    "- **Logistic Regression**: For binary and multi-class classification problems\n",
    "- **SGD (Stochastic Gradient Descent)**: Implementations of linear models optimized with gradient descent\n",
    "\n",
    "### Decision Trees\n",
    "- **DecisionTreeClassifier/Regressor**: Tree-based models for classification and regression\n",
    "- **Random Forest**: Ensemble of decision trees trained on random subsets of data and features\n",
    "- **Gradient Boosting**: Sequential ensemble that builds trees to correct errors of previous ones\n",
    "- **AdaBoost**: Focuses on difficult training examples by adjusting their weights\n",
    "\n",
    "### Support Vector Machines\n",
    "- **SVC/SVR**: Support Vector Classification/Regression with various kernels (linear, polynomial, RBF)\n",
    "- **LinearSVC/LinearSVR**: Faster implementations for linear kernel cases\n",
    "\n",
    "### Naive Bayes\n",
    "- **GaussianNB**: For continuous data assuming Gaussian distribution\n",
    "- **MultinomialNB**: For discrete count data (e.g., text classification)\n",
    "- **BernoulliNB**: For binary/boolean features\n",
    "\n",
    "### Nearest Neighbors\n",
    "- **KNeighborsClassifier/Regressor**: Classification/regression based on k nearest neighbors\n",
    "- **NearestCentroid**: Classification using the nearest centroid\n",
    "\n",
    "### Clustering: Unsupervised\n",
    "- **KMeans**: Partitions data into k clusters by minimizing within-cluster variance\n",
    "- **DBSCAN**: Density-based clustering that can find arbitrary-shaped clusters\n",
    "- **Hierarchical Clustering**: Builds nested clusters by merging or splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bc5707-cd80-4669-89ed-7a6114274915",
   "metadata": {},
   "source": [
    "## Getting Data\n",
    "\n",
    "Before we discuss any algorithms, we will discuss how to get data and how to properly prepare your data into a training set, validation set, and test set. We will also discuss why we would wish to split the data in this way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8856a900-60cc-41f2-8c77-5435db4c41a8",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "Before we can train any algorithms, we will need some data. SciKit Learn's `datasets` module contains a number of sample datasets that can be used to test your code, or to benchmark an approach and so on. \n",
    "\n",
    "Benchmarking datasets are useful as you can compare your results of a new approach that you have undertaken, as you can directly compare metrics with the state of the art, etc. \n",
    "\n",
    "Often, benchmarking datasets have pre-defined training and test sets, so that you can compare your approach's evaluation metrics with the metrics from the identical test set of previous state-of-the-art approaches. \n",
    "\n",
    "So let's now have a look at the datasets API of SciKit-Learn.\n",
    "\n",
    "Perhaps not surprisngly, the `sklearn.datasets` package contains the relevant functions and so on.\n",
    "\n",
    "There are 3 main categories of the `sklearn.datasets` package:\n",
    "\n",
    "1. Toy datasets\n",
    "2. Real-world datasets\n",
    "3. Data generators\n",
    "\n",
    "Some of these are dataset **loaders**, that is the data is included in the software package, and can be immediately loaded into memory. Others are dataset **fetchers**, that is to say that the first time you load such a dataset, the dataset is downloaded from the internet and saved to a directory on your computer. This directory is checked every time you fetch such a dataset, and if it has already been downloaded in the past, it will use the previously downloaded copy. This directory is normally something like `/home/username/.sklearn/datasets` on a Linux system, and will be different under Windows.\n",
    "\n",
    "Both **fetchers** and **loaders** all return a dictionary-like object with *at least* the following attributes:\n",
    "\n",
    "- A `n_samples` $\\times$ `n_features` matrix under the key `data`\n",
    "- An array of length `n_samples` containing the targets, under the key `target`\n",
    "\n",
    "Most of the datasets allow you to specify that you only wish to receive the data and no additional information (most datasets contain information about the class names, and so on), and this can be specfied using `return_X_y=True` when loading the dataset. Most datasets can also be returned as a Pandas DataFrame, you can specify the `as_frame=True` parameter.\n",
    "\n",
    "On the other hand, the **data generator** functions create datasets according to your requirements. This is very useful to test a method or pipeline you have developed. For example, you can create a dataset which is gauranteed to be linerally seperable. If your approach does not work on such a dataset, you can be sure that something in your approach is not functioning.\n",
    "\n",
    "The **data generator** functions all return a **tuple** of the form `(X, y)`:\n",
    "\n",
    "- where `X` is a `n_samples` $\\times$ `n_features` matrix, and\n",
    "- where `y` is an array of length `n_samples`\n",
    "\n",
    "We will look at generating some data later.\n",
    "\n",
    "## Diabetes Dataset\n",
    "\n",
    "As an example of a built-in dataset, which is part of SciKit-Learn by default, we will take a look at the diabetes dataset.\n",
    "\n",
    "First we load the `datasets` module and then use the `load_diabetes()` function to get the data. This data is built-in, and does not need to be fetched from the Internet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c5f4d3-e7f1-460c-a0bd-79532231dac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "diabetes = datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4828bd69-776d-4887-8c62-7598d228d565",
   "metadata": {},
   "source": [
    "The `diabetes` object that is retruned is a dictionary-like object. We can take a look at its keys, to see what kind of data is stored in the the object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f4a5ea-e906-43d7-8e9c-69cec5ac459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe3e357-aaac-47bf-841c-c4706797cb49",
   "metadata": {},
   "source": [
    "We see that we have a `data` attribute, a `target` attribute, and a `DESCR` attribute, and so on.\n",
    "\n",
    "We can just execute the object directly to see everything it contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e01b55-7839-406b-b8c6-3fcdaeb4c3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a78687c-8852-4166-ad89-c43083042cb2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We can see the different attributes, such as the `data` and a `target` key/attributes that we spoke about earlier. The data stored in these attributes can be accessed using a key like a dictionary, such as `diabetes['target']`, or using the `.` notation. \n",
    "\n",
    "Let's have a look at the `DESCR` which should tell us about the dataset itself - note that we can see `DESCR` above, but by wrapping it in a `print()` function it will be displayed in a much cleaner way: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d888b3-3fc0-4cca-ac7e-77a4df4e1c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diabetes.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d559358-64bf-4117-b8ae-541a3e3162ab",
   "metadata": {},
   "source": [
    "Most datasets built in to SciKit-Learn will have this `DESCR` attribute.\n",
    "\n",
    "We can also take a look at the targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5589a98-0406-4d25-8dc3-aa74561dcfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ca5f3c-d764-45a1-96b6-b14ba4bc87be",
   "metadata": {},
   "source": [
    "These values are the values you would try to predict in a machine learning algorithm, while `data` contains the features for each patient.\n",
    "\n",
    "We can preview the data here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b82054-c4dc-4cde-aeb0-140f7d9df15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diabetes.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2727416-bfaa-4bf3-9afd-20d137e558bf",
   "metadata": {},
   "source": [
    "As we mentioned previously, `target` is an array of length `n_samples`, while `data` is an `n_samples` $\\times$ `n_features` matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c3200c-5943-4e08-8f1a-f41437f96103",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Targets shape: {diabetes.target.shape}\\nData shape: {diabetes.data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8e3355-d836-458d-9d7d-5de214578b7a",
   "metadata": {},
   "source": [
    "The feature names are also available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51283b2e-7cf4-407c-b38c-f377c4c2ec37",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dff196f-a1b6-4944-a492-dafb975133c5",
   "metadata": {},
   "source": [
    "The values `s1`, `s2`, etc. are not very descriptive, however they are described in the `DESCR` attribute, e.g. `s1` is described as the total serum cholesterol.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07a2753-5004-4008-9d65-291409370fa7",
   "metadata": {},
   "source": [
    "Most datasets can also be accessed as Pandas DataFrames, which is often easier to preview within environments like Jupyter.\n",
    "\n",
    "Therefore, to request a DataFrame we set the `as_frame` parameter to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e339a6c5-1724-402f-babd-09e5408a1dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df = datasets.load_diabetes(as_frame=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3f6432-086e-4fcd-a1ea-fa22a67a2ce7",
   "metadata": {},
   "source": [
    "Now wecan preview the `data` attribute (which is now a Pandas DataFrame) using the `head()` or `tail()` functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5173d536-9f17-4499-b19d-863045df2957",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df.data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25f7b60-61e9-4dba-85c9-af85c140a3ce",
   "metadata": {},
   "source": [
    "Having a Pandas DataFrame instead of a Numpy 2D array, does make some things a bit easier, such as being able to select columns/features by their column name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4667a8b7-8c27-4b3c-9bc1-b13a5f5f66cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df.data.bmi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe00f4c-a5aa-438e-a72d-da514061fdac",
   "metadata": {},
   "source": [
    "Pandas includes other helper functions include `describe()` which prints some statistics about each feature in the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526cbbe6-f3a4-440b-8815-b6e53dfb63de",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df.data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3316e2eb-5de7-4ae6-9d92-e40e3a29d1f5",
   "metadata": {},
   "source": [
    "Which you prefer depends on your preferences or use case. If you are working within Jupyter, it can often make sense to use DataFrames, however, if you are writing scripts with zero user interaction, then often Numpy is the better choice. \n",
    "\n",
    "If you are not interested in any of the additional information, such as the `DESCR` or anthing else, you can specify `return_X_y=True` parameter, which returns nothing but the raw data and the targets, which you can save in to `X` and `y` directly, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eb95e4-1638-4957-b1d7-403bd8f9586e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d3858a-aeff-4b42-933a-c9c31eba2eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c2345f-40ad-44ed-8485-a276cbc305e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b0bf14-1c99-4982-b0f0-7eb929db596a",
   "metadata": {},
   "source": [
    "This returns only the `X` and `y` data structures and nothing else, by default as Numpy arrays. You can also combine this with the `as_frame=True` to get `X` and `y` as DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b6a71e-6815-4a26-a484-6d78c3274b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.load_diabetes(return_X_y=True, as_frame=True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad12f005-e598-44e3-a615-40583754211b",
   "metadata": {},
   "source": [
    "## Fetching Data\n",
    "\n",
    "As mentioned previously, SciKit-Learn also allows you to fetch data from the Internet. One such option is the OpenML repository, and in this section we will take a look at SciKit-Learn's API for interfacing with <https://openml.org>.\n",
    "\n",
    "If we take a look at the website now, and search for something like 'mice', we will see the types of data available...\n",
    "\n",
    "The website <https://openml.org> gives you access to over 6,000 datasets, and SciKit-Learn allows you to download them directly, and in a format that SciKit Learn can use immediately.\n",
    "\n",
    "To do this we use the `fetch_openml()` function. \n",
    "\n",
    "First we import it from the `datasets` module, and then download the dataset we want using `fetch_openml()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06541a6-e1c8-4285-ad33-39e6fd8c4f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mice = fetch_openml(name='miceprotein', version=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51ebfdf-f225-4b2d-bf9b-df94883ec354",
   "metadata": {},
   "source": [
    "If we access `mice` directly, we will see it follows the same format as the built-in `diabetes` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c84a5d-a1c0-4fde-9e39-9d04437ba728",
   "metadata": {},
   "outputs": [],
   "source": [
    "mice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd516ff-1595-4fe2-9208-62440d43b3ad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Just as we saw with `diabetes` above, we have `data` and `target` structures available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2379dae8-a067-47cf-be80-e15cc9679a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "mice.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869dcc71-562d-42f2-bb33-4d7999490012",
   "metadata": {},
   "outputs": [],
   "source": [
    "mice.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a04cd4-c73d-4655-acb7-6a861a030aa9",
   "metadata": {},
   "source": [
    "In the case of the `MiceProtein` dataset, there is a `details` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893e3978-ba79-4336-a648-28ebdea081e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mice.details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4205e4a-a2fd-4810-974e-478b2aba4ef8",
   "metadata": {},
   "source": [
    "To summarise, gathering data can be very tedious. Especially if data is provided in obscure formats, or proprietary formats that require converting to a format we can read with Python or SciKit-Learn. \n",
    "\n",
    "The `datasets` module within SciKit-Learn is a very convenient source of datasets, many of which are used as benchmarking datasets so that you can confirm that your methods work to a sufficient standard. It also allows you to pull data from online resources. \n",
    "\n",
    "Last, we will look at generating data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77049577-070a-494e-95e6-cdc8313fb9e9",
   "metadata": {},
   "source": [
    "# Generating Data\n",
    "\n",
    "Sometimes, however, you want a dataset with very specific properties to test a machine learning approach that you are trying. Here is where you might use a data generator instead.\n",
    "\n",
    "Generators are part of the `sklearn.datasets` package, generally preceded with the word `make_...`. For example `datasets.make_classification()`. \n",
    "\n",
    "If we want to create a classification dataset, we can do so as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32498455-7fba-489d-a4b7-eeef8cce9c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d3d298-a76b-4938-816b-800004105db9",
   "metadata": {},
   "source": [
    "The `make_classification()` function is quite involved.\n",
    "\n",
    "We can take a look at the documentation, to get a rough idea of what types of data can be generated using it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109bf7b5-b02f-4c32-b3a0-5d52d1424954",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46c8a91-3dd6-45d5-9a75-42a38816ec78",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Basically, `make_classification` creates a random $n$-class classification problem. \n",
    "\n",
    "If we generate a very simple dataset, we can visualise this easily. \n",
    "\n",
    "Using the default values, we can run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb3c130-99c0-4116-9c41-3fe83863d393",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(random_state=0)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fc671f-4b1c-4ea5-9a20-63ec149ea97e",
   "metadata": {},
   "source": [
    "As can be seen, `make_classification()` returns a tuple containing your data in `X` and your targets in `y`. This is exactly how most of the data is returned by many of the `dataset` module's functions.\n",
    "\n",
    "The data stored in `X` contains 100 samples, each with 20 features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab7a446-11e2-4a98-bb05-242e8507fb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c61412-14c6-48c7-a0d9-5cfc76c21675",
   "metadata": {},
   "source": [
    "Because this is generated data, the data doesn't contain feature names, and the data looks quite randnom.\n",
    "\n",
    "We can plot a few of the features, to get an idea how it looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df447868-607e-41fc-8561-e26086920ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df6dc4b-458a-48a0-8e4a-df9dd64db6e1",
   "metadata": {},
   "source": [
    "Remember from earlier that `X[:,0]` returns all rows from column 0 and that `X[:,1]` returns all rows from column 1. We use `c=y` to define the colours of the classes, and colour the points on the plot.\n",
    "\n",
    "This means the plot above only plots feature 0 versus feature 1.\n",
    "\n",
    "We a can define exactly how we want to create the data, for example we can say that we want 2 features in the dataset, and both are informative. This should then produce a dataset that should be trivially seperable using any basic algorithm. \n",
    "\n",
    "Let's create this more simple dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedebb3b-0b1a-403a-88a0-e90f9be7db1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=100, n_features=2, n_redundant=0, n_informative=2, random_state=2, n_clusters_per_class=1)\n",
    "plt.scatter(X[:,0], X[:,1], c=y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b435e0-a83b-4253-9fc9-593aa877bbc8",
   "metadata": {},
   "source": [
    "This looks much better and should be linearly seperable. \n",
    "\n",
    "As you can see, you can create a dataset to match your exact needs, and thereby create data that should be, for example, linearly classifiable. \n",
    "\n",
    "You can of course also make data for regression tasks, unsupervised tasks, inlcude noise, and so on.\n",
    "\n",
    "## Summary of Data Gathering\n",
    "\n",
    "We have seen several ways in which we can gather data. However, getting data is really only half the story. Once data has been gathered, we normally need to prepare it in some way and often data needs to be 'cleaned'. Exactly what this means will be discussed in the next section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430c6fd7-60db-4aae-bb5d-6a5f2a2b3705",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Let's briefly discuss the topics of data preprocessing.\n",
    "\n",
    "What is data preprocessing? It is merely getting a dataset ready to be inputted in to a machine learning algorithm for training. Exaclty what you need to do often actually depends on the type of algorithm you are training. For example, one simple type of data cleaning is handling missing values. Some algorithms cannot handle missing data for example, while others can. Therefore, this step would depend on the algorithm you want to train.\n",
    "\n",
    "Other operations include scaling your data, using feature scaling. Sci-Kit Learn provides numerous helper functions for such tasks, as we will see here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa94ff3-0cd8-4e03-9aef-fef045f3dc66",
   "metadata": {},
   "source": [
    "## Cleaning Data\n",
    "\n",
    "Before we scale or normalise the data, we might want to clean it first. Threfore let's clean a very simple dataset right now to see some methods that you might use.\n",
    "\n",
    "Note that it is often much easier to use Pandas for such tasks. You can always convert your dataset to Numpy if required later.\n",
    "\n",
    "Let's work with a dataset known as the Cleveland Heart Disease Dataset, a well known dataset used for benchmarking.\n",
    "\n",
    "With Jupyter we can view the data directly, which we will do now! \n",
    "\n",
    "**Note** that we know from looking at the dataset, that missing values are represented by `?` characters.\n",
    "\n",
    "Let's load the dataset and then preview it. \n",
    "\n",
    "Pandas provides methods for loading CSV files, Excel files, and so on. In this case we are loading a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b1a12e-0423-4ada-b06c-276c0d7862f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "heart_disease = pd.read_csv('./data/cleve.mod.mdb.csv', na_values='?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a44497-66a2-4b48-917d-a0d189770525",
   "metadata": {},
   "source": [
    "Now that it has loaded, we can use the `info()` function to have a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae11135-5ad1-4ec8-b9bb-ddddf7574742",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f467fcb9-e053-41ee-b3eb-5a7b65abd36e",
   "metadata": {},
   "source": [
    "We have 303 rows/patients, and 14 features. The tpyes of the features are also visible. For example, Age is a numerical field, while Blood Sugar Less 120 is a boolean field.\n",
    "\n",
    "You may also see that Number of Vessels Coloured has 298 non-null values...\n",
    "\n",
    "We can also preview the data using `head()` or `tail()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107f5c7a-dd32-437c-ae00-277b483cb3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4edd3e7-c147-471d-b342-29d5974a03b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(heart_disease.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a6cd76-e6d3-47e1-a7d9-6fa1070aa360",
   "metadata": {},
   "source": [
    "### Cleaning\n",
    "\n",
    "Checking for missing values is one of the **first things** we will do when examining a dataset.\n",
    "\n",
    "We can do this using the `isnull()` function, and print the missing values per feature/column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ccc62b-917a-4680-b8d6-1030b807d258",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMissing values per column:\")\n",
    "print(heart_disease.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e438635-65f2-40b2-8a12-d751ffbc3411",
   "metadata": {},
   "source": [
    "We can look at the rows using the `isna()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd771763-b98b-438f-824c-09188a087536",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease[heart_disease.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff3a018-3538-40fa-b172-8619b997326b",
   "metadata": {},
   "source": [
    "Missing data is represented by a special typed called `NaN` (not a number). We see the rows where Number of Vessels Coloured and Thalassemia have missing values.\n",
    "\n",
    "Many algorithms cannot handle missing data (although some can), and therefore we have to handle this somehow.\n",
    "\n",
    "An easy way to do this would be to simply drop those rows. After, only a few rows have missing data. \n",
    "\n",
    "However, we can also replace the missing values by, let's say the mean value for that column. \n",
    "\n",
    "So, for example, a patient with a missing age would get the average age of all the patients.\n",
    "\n",
    "In Pandas we can do with in one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f4b4b7-a7c0-4991-a87f-47f3161948ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease = heart_disease.apply(lambda x: x.fillna(x.median()) if x.dtype.kind in 'ifc' else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07941f1c-c598-47f5-81fc-157d49d7c1f8",
   "metadata": {},
   "source": [
    "In this case, we are using the `fillna()` function to replace NAs with the median, if the type is `i` (integer), `f` (float), or `c` (complex). The `apply()` function runs a function across the enture dataset, and we use a lambda function to define this inline.\n",
    "\n",
    "Now let's see if we have any missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2320a747-73d2-44c8-90af-91c9f006a8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(heart_disease.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cadbeb-515b-47b8-afac-717f1381f88e",
   "metadata": {},
   "source": [
    "We can see that Thalassermia still has 2 missing values. We can check again check these rows using the `isna()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0cc6d1-a18e-41d0-b278-7ae44d0c4af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease[heart_disease.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43952a6-2a3f-41e3-907c-0b1df365a9db",
   "metadata": {},
   "source": [
    "The reason that these were not replaced is because Thalassemia is not a numerical value, and therefore cannot be replaced using a mean or median. \n",
    "\n",
    "We could replace them with the most frequently occuring value, however in this case we will do what is also often done, and we will remove the rows entirely. There are only 2 rows out of over 300 so it should not impact our training.\n",
    "\n",
    "Dropping rows with missing data is so common, there is a function that does exactly this called `dropna()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1e73ba-a8ac-4873-b1fc-149bfea12415",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease =  heart_disease.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c60413-15fa-4062-9287-f4faff735c47",
   "metadata": {},
   "source": [
    "We can check if we have an missing values again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1add38a-dc74-47de-b12e-e3df49cdf708",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(heart_disease.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5cd813-6788-41bf-b770-9329f45c7090",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb68a5cf-b154-4a10-b219-67add333f315",
   "metadata": {},
   "source": [
    "Missing values are now all removed.\n",
    "\n",
    "You may have noticed that we have a Status and Status Type field. Normally by convention the last column is our target, but in this case Status is our target and Status Type is irrelevant, as it does not tell us anything about the patient (it relates to how the data was collected and is therefore useless for the actual classification of the patient).\n",
    "\n",
    "Therefore we will drop this column altogether. We can use the `drop()` function for this, which allows you to specify what you want to remove, and which axis this is on. Remember axis 0 are our rows, and axis 1 are the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8a456c-c84e-4e62-a1ef-022eaa72891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.drop('Status Type', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacd6910-38f7-4c65-b25a-2e3d9421b9e5",
   "metadata": {},
   "source": [
    "By saying `inplace=True`, we modify the `heart_disease` DataFrame directly, otherwise Pandas would return a new DataFrame with the data removed. Many Pandas functions do this, so you need to be careful about this.\n",
    "\n",
    "Therefore, we should now see one less column/feature in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfd759d-53ef-4a89-a1c1-1f33b29a6ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5601343a-7754-4180-9e7e-95c8fdc14fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9491fa64-9880-4835-b7d0-eea2fa88918e",
   "metadata": {},
   "source": [
    "We may also have noticed that the **Status** column contains the values 'healthy' and 'sick' and relates to **presence** or **absence** of heart disease. \n",
    "\n",
    "Maybe, we decide that the terms 'healthy' and 'sick' are not how we would like to phrase it, so we can replace these values with 'absent' and 'present' instead.\n",
    "\n",
    "For this we will use the convenient `map()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20f86f0-391d-4edb-b7bf-5fed3e47c598",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease['Status'] = heart_disease['Status'].map({'healthy': 'absent', 'sick': 'present'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37683f56-61be-4bb5-86a7-4d267709df08",
   "metadata": {},
   "source": [
    "Preview our data once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecaeef9-8e31-4010-a5d3-267be0e1d3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e971542-f77b-4fc8-8377-aa2b9eb73231",
   "metadata": {},
   "source": [
    "This is just to demonstrate how to replace values quickly, in fact once we input the values into an algorithm, these strings will be replaced by numerical/boolean values anyway. \n",
    "\n",
    "You will also notice, that several of the columns in the dataset contain categorical values in the form of strings, such as gender.\n",
    "\n",
    "We can use `unique()` to see how many values a column has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c395b88-c13e-4560-b3c3-792a4fbaf6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(heart_disease.Gender.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f2d870-32c1-48c1-814d-6d1092f11cac",
   "metadata": {},
   "source": [
    "For input in to a training algorithm, we cannot use such categorical values. We need to replace these with numerical values.\n",
    "\n",
    "We can replace values quite easily, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73f048d-45d3-4e94-87dd-f083b428d6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.Gender = heart_disease.Gender.replace('M', 0)\n",
    "heart_disease.Gender = heart_disease.Gender.replace('F', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b576416b-f3e0-4b9c-a9da-32ce4e8da157",
   "metadata": {},
   "source": [
    "Here we use Pandas' `replace()` function to do this. This is perhaps even easier than using `map()`, from above.\n",
    "\n",
    "**Note**: if you see a warning when we run this function, we can safely ignore it for now - it is merely informing us about a future change to the functionality of the `replace()` function.\n",
    " \n",
    "We can look at our data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079d5a03-0ba3-4cce-91fa-191803d44ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c913e81a-e77b-4fe1-9c9a-cba686f7152d",
   "metadata": {},
   "source": [
    "The gender field only had 'M' and 'F' values, so we could just replace these manually using two lines of code. However, if a field was to contain many different values, you can loop over them and replace them. \n",
    "\n",
    "We will do this now to demonstrate how this works.\n",
    "\n",
    "Again we use the `unique()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864efbdb-8fba-4d6f-aa72-e9fa9e2f2016",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpt = heart_disease['Chest Pain Type'].unique()\n",
    "print(cpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4296ed0-517a-40eb-8b32-61e5df8c9ab7",
   "metadata": {},
   "source": [
    "We now have a list containing the unique values for this field. \n",
    "\n",
    "We can replace them with numerical values as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6840f593-ebba-4e29-9598-f15d4edbe92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, value in enumerate(cpt):\n",
    "    heart_disease['Chest Pain Type'] = heart_disease['Chest Pain Type'].replace(value, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf064b3e-bda0-402f-bdcb-f19c0642bd19",
   "metadata": {},
   "source": [
    "And once again preview our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e41f1-d9ab-4052-af51-dfc78be21c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92619c75-0b13-4321-ad1b-e924e6d143bb",
   "metadata": {},
   "source": [
    "However, this method is quite manual and not very clean, and we need to ensure we keep our `cpt` list otherwise we will not be able to trace the numerical values back to the categories. A better approach is to use one of SciKit-Learn's encoders.\n",
    "\n",
    "So instead, for the remaining categorial features, let's use SciKit-Learn's `OridnalEncoder` to do this.\n",
    "\n",
    "For demonstratin purposes, let's first do this to 'Resting ECG' column to see how it works.\n",
    "\n",
    "Let's import `OrdinalEncoder` and then create a new object called `encoder`, telling it to transform the 'Resting ECG' feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c67f9-98f8-489e-85ab-bdda9f61ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "encoder = OrdinalEncoder().set_output(transform=\"pandas\")\n",
    "\n",
    "# Encode the Resting ECG column\n",
    "resting_ecg_encoded = encoder.fit_transform(heart_disease[['Resting ECG']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135ec73c-82fb-496e-979e-0416b739c9c1",
   "metadata": {},
   "source": [
    "This is now done, and we can take a look at our encoded column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b6dd38-ae63-4f58-8f5e-561016c24e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "resting_ecg_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c473f6-e59a-4b78-b521-23d59c06af46",
   "metadata": {},
   "source": [
    "As you can see, the categorical values have been given numerical values for the 'Resting ECG' column.\n",
    "\n",
    "In this case, we did this just on one column, to demonstrate how it works - but we can apply the encoder to multiple columns at the same time and perform this conversion with just a couple of lines of code. So let's do this now.\n",
    "\n",
    "First, let's create a new encoder, and then tell it the categorical featurres that we wish to encode as numerical values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602e5adc-7aa6-4feb-afa5-6f05098888bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OrdinalEncoder().set_output(transform=\"pandas\")\n",
    "\n",
    "encoded = encoder.fit_transform(heart_disease[['Resting ECG', 'Slope', 'Thalassemia', 'Status']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083d8cf1-a2bc-412b-a666-0461be17862d",
   "metadata": {},
   "source": [
    "That's it, in just a few lines of code we have converted 4 columns in to numerical features!\n",
    "\n",
    "We can take a look at the columns now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16337c19-706d-41e6-9f2d-f1db1b5f6f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df31b181-02d2-44fa-bfc5-03a331d822fe",
   "metadata": {},
   "source": [
    "Now that we see they look good, we can replace our columns with the encoded columns in one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d6734e-bd63-4ceb-92de-4f4bee21073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease[['Resting ECG', 'Slope', 'Thalassemia', 'Status']] = encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dce034-d09f-4f41-91ed-5437df533807",
   "metadata": {},
   "source": [
    "Let's preview our data once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce30429-53b4-4fea-9896-2e2d8115bd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdf3f36-c6a2-4181-80df-c372e5140e00",
   "metadata": {},
   "source": [
    "Now all our data is numerical or Boolean (which can be handled as numerical data by Sci-Kit Learn, as they simply represent 0 and 1 anyway).\n",
    "\n",
    "Note, a very useful feature is that the `encoder` object contains our original categories, which we can reference later if we needed to know what 1 means in the 'Status' field, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a95f430-8b39-4a7d-ab57-ecc71cb53b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923bddd3-4519-469c-915a-232ddcb445fa",
   "metadata": {},
   "source": [
    "### Ordinal vs. Nominal \n",
    "\n",
    "We have assumed in each case above that there is an inherent order to the data, hence using an Ordinal encoder. \n",
    "\n",
    "A few things to note however: \n",
    "\n",
    "- **First**, we would need to check if these fields are indeed ordinal or nominal. For example, the Gender field has been encoded as Ordinal, meaning there is some order to gender where female, which is encoded as 1, is somehow 'worth more' than male which is encoded as 0. This is true for the Resting ECG field, as `norm` is *better* than `hyp` which is *better* than `abn`.\n",
    "- **Second**, we did not actually specify the order for the ordinal encoder, we would normally do so by telling the encoder that `norm` should be `0`, `hyp` should be `1`, and `abn` should be `2`. We didn't do this in order to simplify the code somewhat.\n",
    "\n",
    "Therefore, let's fix the Gender field using a nominal encoder. Nominal encoding assumes no ranking of values. Other fields like this might be city, or state, etc.\n",
    "\n",
    "To do nominal encoding you need to actually create a new feature for each of the possible values in the original field. \n",
    "\n",
    "So, instead of Gender being encoded like so:\n",
    "\n",
    "|   | Gender |\n",
    "|---|--------|\n",
    "| 1 | 1      |\n",
    "| 2 | 0      |\n",
    "| 3 | 0      |\n",
    "\n",
    "We would encode it as so:\n",
    "\n",
    "|   | Gender_Female | Gender_Male |\n",
    "|---|---------------|-------------|\n",
    "| 1 | 1             | 0           |\n",
    "| 2 | 0             | 1           |\n",
    "| 3 | 0             | 1           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1e914f-dcef-4140-b5c4-d6b805b72807",
   "metadata": {},
   "source": [
    "To do this, we create a nominal encoder which is called a `OneHotEncoder` in machine learning parlance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e47c112-53d1-4b3d-8349-d915cec98c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False).set_output(transform=\"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8378e5b8-c1dd-4141-82ab-e363cd8a6965",
   "metadata": {},
   "source": [
    "Now perform the encoding, and preview it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c99880-cf02-4604-b317-c6f2610bc993",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_gender = encoder.fit_transform(heart_disease[['Gender']])\n",
    "encoded_gender.columns = [\"Gender Female\", \"Gender Male\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e3c61d-ea46-4d4c-b406-4b8817976c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_gender"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285e9afe-b2c5-4eb7-a993-74c54cee9070",
   "metadata": {},
   "source": [
    "Now we can insert our new columns. First we drop Gender:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fd8c9a-2410-4798-8165-0223721fdbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.drop('Gender', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5fc201-3173-4777-8617-abdfe4432224",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd399d71-1898-4e1e-943c-caeb35a5dd17",
   "metadata": {},
   "source": [
    "And add our new columns using the `insert()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f0320a-33b3-4211-a880-e24afd4d1663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the first column at index 1\n",
    "heart_disease.insert(loc=1, column='Gender Female', value=encoded_gender['Gender Female'])\n",
    "\n",
    "# Insert the second column at index 2\n",
    "heart_disease.insert(loc=2, column='Gender Male', value=encoded_gender['Gender Male'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d3b611-802b-4847-b4c6-58efe37f5d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c6a419-de86-4c25-a489-eeade9a4b8c3",
   "metadata": {},
   "source": [
    "## Scaling and Normalising\n",
    "\n",
    "Now that we have dealt with our categorical values, we need to scale some of our fields. \n",
    "\n",
    "Scaling means that we alter our numerical so that it looks like normally distributed data, with a mean of 0 and a standard deviation of 1. Some methods scale all values between 0 and 1, while others scale between -1 and 1. \n",
    "\n",
    "Why is this done? Quite a few algorithms will behave non-optimally if numerical data is not scaled, because different features might be deemed as more important because they have higher values. For example, in our heart disease dataset, we might incorrectly consider Cholesterol are having a larger impact thant Rest Blood Pressure because the values tend to be much higher. If we scale the values between 0 and 1, then each feature looks as important as any other feature. \n",
    "\n",
    "SciKit Learn has a large number scaling features built in to its `sklearn.preprocessing` module. \n",
    "\n",
    "Here we will use the `StandardScaler` to demonstrate how to do feature scaling.\n",
    "\n",
    "First let's import `StandardScaler` and create a `scalar` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f64fe4-5418-4fc4-9698-bd1cf73a19b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().set_output(transform=\"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a5aad8-f4aa-47a7-886e-af923a7268a6",
   "metadata": {},
   "source": [
    "Using this `scaler` we will now scale one feature, namely the Cholesterol column, to demonstrate its usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7e1f79-c7e0-44fe-ab16-9801bb51a802",
   "metadata": {},
   "outputs": [],
   "source": [
    "cholesterol_scaled = scaler.fit_transform(heart_disease[['Cholesterol']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdc7b7c-c706-492b-b2f6-1a43e7ef8f53",
   "metadata": {},
   "source": [
    "Now the data has been scaled, we can look at it side by side with the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793edc47-e79d-44ad-857f-347354da3531",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([cholesterol_scaled, heart_disease.Cholesterol], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01984fe-fd74-43e1-81f2-33559f239175",
   "metadata": {},
   "source": [
    "We can replace our Cholesterol field with our new scaled version as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9093de3-35db-4c91-aa66-02c9b1c035be",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.Cholesterol = cholesterol_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b043a0e0-3fe8-44db-a3a9-7f89461a030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca97be3-8c77-4a2b-8107-70b9bf6dab7a",
   "metadata": {},
   "source": [
    "We can do the same with Age, Resting Blood Pres, Max Heart Rate, and Old Peak.\n",
    "\n",
    "To shorten the code, we can just apply the `fit_transform()` and replace the original column in one go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47926b4-3685-4d32-ad24-f9142c2641bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease['Age'] = scaler.fit_transform(heart_disease[['Age']])\n",
    "heart_disease['Resting Blood Pres'] = scaler.fit_transform(heart_disease[['Resting Blood Pres']])\n",
    "heart_disease['Max Heart Rate'] = scaler.fit_transform(heart_disease[['Max Heart Rate']])\n",
    "heart_disease['Old Peak'] = scaler.fit_transform(heart_disease[['Old Peak']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0bd6d3-23e7-467f-acef-cf89379afcd6",
   "metadata": {},
   "source": [
    "And then preview our data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8565cd-228c-4002-989d-22e3ef645afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4322283a-4938-4370-9364-b5b7b0df7172",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "We will not deal with this directly, but another cause of issues, even if you have scaled the data, are outliers. \n",
    "\n",
    "These are values which are far outside of the mean value for a particular feature. \n",
    "\n",
    "Going back to the scaled Cholesterol feature, we can plot the distrubution of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a167768-5f35-41a2-987a-4e6bbf1deee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cholesterol_scaled.plot.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f0da48-f538-4269-9af6-552608b5ff11",
   "metadata": {},
   "source": [
    "Seems we might have an outlier or two at around the 6 mark which has skewed things somewhat. Maybe a boxplot would tell us more?\n",
    "\n",
    "Pandas has a a large number of plotting tools, including the ability to generate a boxplot very easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad10e1b-a9c4-440a-9b34-19b0c5b55dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cholesterol_scaled.plot.box();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc2e2d3-17b0-47bc-977f-d45482e49058",
   "metadata": {},
   "source": [
    "As a reminder, the green line shows the median value, meaning 50% of the data is above that line and 50% below. The circles show outliers. The blue lines show the upper and lower quartiles.\n",
    "\n",
    "There are several techniques with how to deal with outliers, however we will not deal with them in this example. If you want to account for outliers, that take a look at `QuantileTransformer` or `PowerTransformer`, see <https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html>\n",
    "\n",
    "**Note**, that not all algorithms require scaling. Some, however, absolutely require that all features centre around 0. Check the documentation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca77e597-baa6-4e6f-bedc-98e91f9a1093",
   "metadata": {},
   "source": [
    "### Summary of \n",
    "We have seen how we can scale and encode features quite easily. Which type of encoder you use depends on the data and whether it is nomninal data, such as gender, or ordinal data, such as Chest Pain Type, which clearly has an order. \n",
    "\n",
    "Once you are used to the SciKit Learn `preprocessing` module, you will see that most scalers, normalisers, and encoders have a very similar API, and that you will be able to transform entire datasets in just a few lines of code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0581c6f3-bdc9-4f6e-b3e5-332c71445942",
   "metadata": {},
   "source": [
    "### Final Steps\n",
    "\n",
    "Finally, our dataset is ready and can be saved to the conventional Numpy format for SciKit Learn.\n",
    "\n",
    "First we create our label array, which is the Status field, and then drop it from the DataFrame, and also take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78623893-cb45-4e92-8328-427e511b0ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y = np.array(heart_disease[\"Status\"])\n",
    "heart_disease.drop('Status', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5027ed45-436b-41bb-9503-4419c2db26f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab65c41-abd0-4a3a-ab36-13fed88702d1",
   "metadata": {},
   "source": [
    "And now convert our remaining data in to a 2D Numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb956862-f32c-4e4b-b93c-a3e482fc3b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(heart_disease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb50734-b1a0-4219-adfb-fe4da191c066",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ad068d-75f0-48b2-9064-ea80d79070fc",
   "metadata": {},
   "source": [
    "The next step normally would be to create our train/validation/test splits, and then train our model. However, we will do this later, after we discuss a concrete machine learning algorithm first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943e86f3-62ad-48f9-8aee-cd64f7c4294e",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "The first concrete, actual algorithm we will look at is called linear regression and is probably the simplest machine learning algorithm you will encounter.\n",
    "\n",
    "Using this simple example we will take a look at model evaluation, which is how you evaluate how well your model is performing, and how well it will perform on **new data**. This is crucial to understand how well your model might work after it has been trained, and is used on new, unseen data.\n",
    "\n",
    "So, imagine the following scenario - you have a dataset of house prices and area in square metres.\n",
    "\n",
    "We will generate this datset, but creating a linear relationship between size of the house and price of the house, and add some noise to the data.\n",
    "\n",
    "We do so as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1158db-9483-4574-94ec-1a7371746ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Specify the number of samples\n",
    "num_samples = 100\n",
    "\n",
    "# Generate 100 random areas in square meters (from 50 to 250) \n",
    "areas = np.random.uniform(50, 250, num_samples).astype(int)\n",
    "\n",
    "# Generate house prices with a base price, added by a linear function of area (with some noise)\n",
    "base_price = 100000  # base price for the smallest house\n",
    "price_per_sqm = 2000  # price per square meter\n",
    "noise = np.random.normal(0, 30000, num_samples).astype(int) # adding some noise to make it more realistic\n",
    "\n",
    "# Our prices are therefore the base price, plus the area * price per square meter, plus the noise\n",
    "prices = base_price + areas * price_per_sqm + noise\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'Area (sqm)': areas,\n",
    "    'Price': prices\n",
    "})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fba462-86b8-4975-870e-69507006cfb3",
   "metadata": {},
   "source": [
    "We have saved the data as a Pandas DataFrame.\n",
    "\n",
    "To get a better idea of how the data looks, we can plot it. \n",
    "\n",
    "We will use the `scatter()` function from Matplotlib to plot the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c4024b-0895-49b6-b27d-69103375bcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the data\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data['Area (sqm)'], data['Price'], alpha=0.5)\n",
    "plt.title('House Prices vs Area')\n",
    "plt.xlabel('Area ($m^2$)')\n",
    "plt.ylabel('Price (€)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7494b89-46fc-429a-b414-b80169bcebac",
   "metadata": {},
   "source": [
    "So what do we see?\n",
    "\n",
    "We see a more or less linear relationship between the area of the house and the price. \n",
    "\n",
    "The bigger the house, the bigger the price. \n",
    "\n",
    "Mote also there may be many other factors that we are not aware of, such as neighbourhood, or if the house has a pool or not, and so on, which would normally mean we would not see such a linear relationship. \n",
    "\n",
    "What we will do now is to train a Linear Regression algorithm, which will try to fit a line to this data as well as possible, and try to capture the relationship between area and price.\n",
    "\n",
    "Do not worry about the details of the code below for now. Later will train an algorithms line by line. \n",
    "\n",
    "For now we will just train the model on the data we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6eda82-9191-4a39-a5a9-dce73d9aa287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Reshape data for modeling\n",
    "X = data['Area (sqm)'].values.reshape(-1, 1)\n",
    "y = data['Price'].values\n",
    "\n",
    "# Create and fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict values for the same input\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Calculate the errors\n",
    "errors = y - predictions\n",
    "\n",
    "# Plotting the data and the linear fit\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data['Area (sqm)'], data['Price'], alpha=0.5, label='Actual Data')\n",
    "plt.plot(data['Area (sqm)'], predictions, color='red', label='Linear Fit (Linear Regression)')\n",
    "plt.title('House Prices vs Area')\n",
    "plt.xlabel('Area ($m^2$)')\n",
    "plt.ylabel('Price (€)')\n",
    "plt.legend()\n",
    "#for i in range(len(X)):\n",
    "#    plt.vlines(X[i], min(predictions[i], y[i]), max(predictions[i], y[i]), color='gray', alpha=0.5)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65984527-1e31-44be-b1a7-2c6341e13dbb",
   "metadata": {},
   "source": [
    "Once we have fit the model to the data, you can use it to predict the prices of new houses. \n",
    "\n",
    "If you wanted to sell your house today, and it has 150m2 area, we could predict a selling price of about 400,000 based on the line above.\n",
    "\n",
    "How is the line actually fit, however? \n",
    "\n",
    "Well, this is a supervised algorithm, and therefore the error for any predicted line can be calculated by measuring the difference between the prediction, based on the line, and the actual price. \n",
    "\n",
    "This is done by measuring the distance between the line and the true value, and is called a residual.\n",
    "\n",
    "The line is moved around until the sum of the errors (residuals) is minimised.\n",
    "\n",
    "We can plot these residuals easily enough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbad9350-e28d-4430-a1ec-54376dcd8cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the data and the linear fit\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data['Area (sqm)'], data['Price'], alpha=0.5, label='Actual Data')\n",
    "plt.plot(data['Area (sqm)'], predictions, color='red', label='Linear Fit')\n",
    "plt.title('House Prices vs Area')\n",
    "plt.xlabel('Area ($m^2$)')\n",
    "plt.ylabel('Price (€)')\n",
    "plt.legend()\n",
    "for i in range(len(X)):\n",
    "    plt.vlines(X[i], min(predictions[i], y[i]), max(predictions[i], y[i]), color='gray', alpha=0.5)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0068a5d5-6d24-48fe-9e0c-a437c1d7ae0e",
   "metadata": {},
   "source": [
    "So the algorithm can get a numerical value for how well a line fits the data, by summing the errors. It then tweaks this line until it minimises this error.  \n",
    "\n",
    "One such way to get a numerical value for the error is the the mean squared error loss:\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{N}\\sum_i^N (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "We will not get much more in depth about how algorithms work in this course. However, knowing how a basic algorithm works should help us later when we are dealing with other more complex methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51d7ce5-8f1c-4886-be27-40cfc6f96a17",
   "metadata": {},
   "source": [
    "## Non-Linear Data\n",
    "\n",
    "So this is fine for linear data, but what about non-linear data? \n",
    "\n",
    "As the name LinearRegression suggests, this algorithm tries to find a linear reltioship in the data.\n",
    "\n",
    "This might now always be the case of course, so let's generate a not quite linear dataset for the house prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41272fdd-8fbd-4b2c-8ee5-bd1a23fc98e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100\n",
    "\n",
    "# Generate synthetic data\n",
    "square_metres = np.linspace(50, 300, num_samples).astype(int) # Area in square metres\n",
    "noise = np.random.normal(0, 20000, square_metres.shape) # Noise to add some variability\n",
    "noise = np.absolute(noise)\n",
    "prices = ((square_metres ** 2) * 3 + noise).astype(int) # Non-linear relationship, prices rise quickly as area increases\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'SquareMetres': square_metres,\n",
    "    'HousePrice': prices\n",
    "})\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "print(data.head())\n",
    "\n",
    "# Plot the data to visualize the relationship\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data['SquareMetres'], data['HousePrice'], alpha=0.6)\n",
    "plt.title('House Prices vs. Area')\n",
    "plt.xlabel('Area ($m^2$)')\n",
    "plt.ylabel('Price (€)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cb95c8-06b3-4f38-9979-096947bcf56e",
   "metadata": {},
   "source": [
    "Now we can see the relationship is not quite linear, it seems that as area increases, price increases more quickly. \n",
    "\n",
    "Of course, we can still fit a straight line to this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a639cb12-1fa3-4c19-964c-665f818bf645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data\n",
    "X = data['SquareMetres'].values.reshape(-1, 1)\n",
    "y = data['HousePrice'].values\n",
    "\n",
    "# Initialize the linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "#Errors\n",
    "errors = y - y_pred\n",
    "\n",
    "# Plot the data and the linear fit\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data['SquareMetres'], data['HousePrice'], alpha=0.6, label='Actual Data')\n",
    "plt.plot(data['SquareMetres'], y_pred, color='red', label='Linear Fit')\n",
    "plt.title('Linear Fit to Non-linear Data')\n",
    "plt.xlabel('Area in Square Metres')\n",
    "plt.ylabel('House Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "# Error bars\n",
    "for i in range(len(X)):\n",
    "    plt.vlines(X[i], min(y_pred[i], y[i]), max(y_pred[i], y[i]), color='gray', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "# Model coefficients\n",
    "intercept, slope = model.intercept_, model.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffe68f0-5dda-4f3f-8ef9-16ef7588b34e",
   "metadata": {},
   "source": [
    "This does not look bad, however it is difficult to judge visually. What we can do is get a measure for how well the line fit. In this case, we will use the `score()` function which returns the $R^2$ score for the fit.\n",
    "\n",
    "The LinearRegression model above has been saved as the variable `model` and we can get its score using the `score()` function, which by default is the $R^2$ score. This is a score between 0 and 1, where 1 is a perfect fit to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bb7378-c2d0-4b3b-9376-39010f7177e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054e470e-1ffc-4e72-9bae-0363539568d3",
   "metadata": {},
   "source": [
    "This score looks quite respectable (1.0 would be a perfect fit). Perhaps though we could improve the score with a curve rather than a line, hoping that the curve will fit the data more closely and therefore result in a better score.\n",
    "\n",
    "So to do this we can now try to fit a polynomial to the data, in other words a non-linear curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf490ed-32d6-4d8c-8681-e2a33353d3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Set the degree of the polynomial to 2 for quadratic fitting\n",
    "degree = 2\n",
    "\n",
    "# Create a pipeline that first transforms the features into polynomial features, then fits a linear model\n",
    "poly_model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "\n",
    "# Fit the model\n",
    "poly_model.fit(X, y)\n",
    "\n",
    "# Predictions using the polynomial model\n",
    "y_poly_pred = poly_model.predict(X)\n",
    "\n",
    "# Errors\n",
    "errors = y - y_poly_pred\n",
    "\n",
    "# Plot the data and the polynomial fit\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data['SquareMetres'], data['HousePrice'], alpha=0.6, label='Actual Data')\n",
    "plt.plot(data['SquareMetres'], y_poly_pred, color='red', label='Polynomial Fit')\n",
    "plt.title('Polynomial Fit to Non-linear Data')\n",
    "plt.xlabel('Area in Square Metres')\n",
    "plt.ylabel('House Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b6f30e-2873-4fff-b635-521abce099f2",
   "metadata": {},
   "source": [
    "This looks at first glance to be better. Of course, the best way is to compare the metrics, once again we calculate the $R^2$ score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33122fc8-2dc1-4da8-8548-0a08df2e9fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ed3bf2-86e6-4c86-8320-463e6a5b59aa",
   "metadata": {},
   "source": [
    "This results in an even higher $R^2$ score that our straight line fit. So the curve fit the curved data better than the line!\n",
    "\n",
    "So the question you might ask is, how do you select the appropriate type of line? \n",
    "\n",
    "For example, we could increase the degree of the polynomial above and it may fit the data even better - the degree of the polynomial increases the complexity of the curve, and could fit the data even better. In fact, eventually you could add enough complexity to a curve so that it would fit the data exactly. But is this what we want to do?\n",
    "\n",
    "So this leads us to the idea of over-fitting and under-fitting, which we will discuss next. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a15fb8-0321-46d1-aa6c-ab96372c0f81",
   "metadata": {},
   "source": [
    "---\n",
    "# Over and Underfitting\n",
    "\n",
    "- What is over/under fitting\n",
    "- How do we spot it\n",
    "- How do we prevent it\n",
    "\n",
    "## What is over/under fitting? \n",
    "\n",
    "**Overfitting** occurs when a model **learns the training data too well**, capturing noise or random fluctuations in the data. All data contains some degree of noise. As a result, an overfitted model performs **well on the training data** but will perform **very poorly when it is applied to new data**. This is a classic sign of an overfit model. Overfitting typically happens when a model is too complex, for example a polynomial with a high degree (we will see examples again later). Complex models have a higher capacity to fit data, but they can also fit to the noise. A Linear Regression model is not complex enough to overfit to the noise of the datasets that we created above, for example.\n",
    "\n",
    "**Underfitting** occurs when a model is too simple to capture the underlying structure of the data. In this case, the model **performs poorly both on the training data and on any new data**. Underfitting often happens when the model is too basic or lacks the required complexity to represent the underlying relationships in the data.\n",
    "\n",
    "Let's plot some examples of overfitting and underfitting.\n",
    "\n",
    "We will use simulated data and train 3 models. One model will not have the complexity to capture the data't structure and will overfit, another model will fit the data too well, including the data's noise, and will overfit, and one model will fit the data about right.\n",
    "\n",
    "Let's train the models and plot their curves now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266c48ad-8496-4505-afad-ff873e330849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Creating a dataset with some noise\n",
    "np.random.seed(0)\n",
    "X = np.linspace(0, 10, 100)\n",
    "y = np.sin(X) + np.random.normal(scale=0.5, size=X.shape)\n",
    "\n",
    "# Underfitting model (Polynomial of degree 1)\n",
    "underfit_model = np.poly1d(np.polyfit(X, y, 1))\n",
    "\n",
    "# Well-fitted model (Polynomial of degree 3)\n",
    "well_fit_model = np.poly1d(np.polyfit(X, y, 3))\n",
    "\n",
    "# Overfitting model (Polynomial of degree 15)\n",
    "overfit_model = np.poly1d(np.polyfit(X, y, 15))\n",
    "\n",
    "# Plotting the data and the models\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X, y, label='Data')\n",
    "plt.plot(X, underfit_model(X), label='Underfit Model (Degree 1)', color='green')\n",
    "plt.plot(X, well_fit_model(X), label='Well-Fit Model (Degree 3)', color='blue')\n",
    "plt.plot(X, overfit_model(X), label='Overfit Model (Degree 15)', color='red')\n",
    "plt.legend()\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Demonstration of Underfitting, Well-Fitting, and Overfitting')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abdbf8a-038c-4766-a123-9448dd7522b1",
   "metadata": {},
   "source": [
    "We can also plot these are 3 seperate plots, as it might be easier to see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6237f4-d588-4164-92a6-9cce9b19e498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the underfitting model\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(X, y, label='Data')\n",
    "plt.plot(X, underfit_model(X), label='Underfit Model (Degree 1)', color='green') \n",
    "#plt.legend()\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Underfitting (Degree 1)')\n",
    "\n",
    "# Plotting the well-fitting model\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(X, y, label='Data')\n",
    "plt.plot(X, well_fit_model(X), label='Well-Fit Model (Degree 3)', color='blue') \n",
    "#plt.legend()\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Well-Fitting (Degree 3)')\n",
    "\n",
    "# Plotting the overfitting model\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(X, y, label='Data')\n",
    "plt.plot(X, overfit_model(X), label='Overfit Model (Degree 15)', color='red') \n",
    "#plt.legend()\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Overfitting (Degree 15)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0859cc42-bdfe-44ea-9237-7f31de7b7c4b",
   "metadata": {},
   "source": [
    "## How do we Notice Overfitting and Underfitting?\n",
    "\n",
    "How we do know if we are overfitting or underfitting?\n",
    "\n",
    "This is normally done by splitting our data in to training data and test data. \n",
    "\n",
    "We experiment with building our models using the training set, and then we test the model on a test set which the model has never seen before. \n",
    "\n",
    "Then, we analyse the performance of the model on both the training set, and the test set:\n",
    "\n",
    "- If the model performs well on the training data, but performs poorly on the test data you have probably overfit the model to the noise of the training set. You need to reduce the model complexity (polynomial of smaller degree in the example above).\n",
    "- If the model performs poorly on the training set and test set, then you may be underfitting, in which case you need to increase the model complexity.\n",
    "\n",
    "Splitting the model into a training set and test **simulates** the scenario of having new, unseen data. The test set data is never seen by the algorithm during training. We will use the training set to train our model, and once this is trained, we can test the model using the test set, which the model has never seen before. \n",
    "\n",
    "Looking at the metrics of the models on the training set and the test will help us decide if we are over-fitting under-fitting.\n",
    "\n",
    "Later we will see examples of using training sets and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b449a9-7a95-49f5-8a85-ac48f1d49580",
   "metadata": {},
   "source": [
    "### Creating a Train and Test Split\n",
    "\n",
    "SciKit-Learn's `model_selection` module contains functions, such as `train_test_split()` which make it very easy to create randomised train/test splits, as well as a number of other useful tools for creating training and testing datasets. \n",
    "\n",
    "Our simulated data from above is stored in two structures, `X` and `y` (which is the convention for naming data), where `X` contains the data and `y` contains the labels.\n",
    "\n",
    "To create a training set and test set we use `train_test_split()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efc4ffd-4054-455d-a5fe-478d4beb6399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68eae10-5d41-4a10-bc9d-a8beadb03ebd",
   "metadata": {},
   "source": [
    "This will shuffle your data (and shuffle `X` and `y` in unison, which is obviously very important) and create your training and testing splits. The `test_size` parameter defines the ratio of training to testing data.\n",
    "\n",
    "We will see a live example of `train_test_split()` being used later.\n",
    "\n",
    "We will also discuss further training/test split strategies at the end of this seminar also, including a method known as cross validation.\n",
    "\n",
    "We can confirm the split as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a04bfc-b6d2-4c6a-b532-c709bfe95fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"Original dataset (X) size: {len(X)}\n",
    "Train data (X_train) size: {len(X_train)}\n",
    "Training labels (y_train) size: {len(y_train)}\n",
    "Test data (X_test) size: {len(X_test)}\n",
    "Test labels (y_test) size: {len(y_test)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d68c8b-4d09-4c79-b201-a31d63b3c525",
   "metadata": {},
   "source": [
    "We can also confirm it has been shuffled. \n",
    "\n",
    "First look at the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b47b177-9f98-44f2-a63a-31a0e840628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d84cff5-04ee-4ec5-8364-976271b90004",
   "metadata": {},
   "source": [
    "And now the shuffled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e1a5b0-01be-4eab-a192-acd4829b83f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13874bf-fab2-41bd-b9d1-1ae6eebde201",
   "metadata": {},
   "source": [
    "Once you have these data structures, we will actually train our algorithms using the training data in `X_train` and and test the trained models using `X_test`. The model has never seen the data in `X_test` and it should show us if we are overfitting or underfitting to the data.\n",
    "\n",
    "We will discuss more concretely how to evaluate models in much more detail later tomorrow.\n",
    "\n",
    "Now we will discuss an important aspect of train/test splits, and a technique called Cross Validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333f37ed-6bc3-4fc6-9691-4f1902dc6ec4",
   "metadata": {},
   "source": [
    "# *k*-Fold Cross Validation\n",
    "\n",
    "One issue with using a single train/test split, is that when you split the data, you may get an 'easy' **test set** at random. \n",
    "\n",
    "Therefore, your model performs well on this training set, and also very well on the test set, but then in a real-world setting the model suddenly underperforms.\n",
    "\n",
    "Equally, you might get a very difficult test set. How might a test set be difficult? Let's say you have outliers in the dataset, a 'hard' test set might end up with all the outliers in the test set. This means the model never saw any outliers during training, and now encounters from suddenly during testing.\n",
    "\n",
    "Therefore, your model might perform very poorly on this test set, and you might think that perhaps your model is simply not very good, when in fact it may well be down to this train/test split.\n",
    "\n",
    "What is the solution to this? \n",
    "\n",
    "We could repeat the experiment several times, with different training / test splits.\n",
    "\n",
    "If the model performs well across a few different training and testing splits, you can argue that the model is more robust. \n",
    "\n",
    "How might you do this? \n",
    "\n",
    "We could manually create *n* number of training and test splits, let's say *n*=3, and run the experiment *n*=3 times. Again, if the performance of the model on the test sets are stable, it implies that your training strategy is robust and perhaps you final trained model would generalisable to real world data.\n",
    "\n",
    "This is fine, but the best way is to systematically split the data in to *k* folds, where each fold is a distinct 80/20 split, for example: and 80/20 split implies *k*=5. Commonly you will see 10-fold cross validation, where you will have 90/10 splits or 5-fold cross validation where you will see 80/20 splits.\n",
    "\n",
    "![CV](./img/cross-validation.png)\n",
    "\n",
    "*Source*: <https://www.mltut.com/k-fold-cross-validation-in-machine-learning-how-does-k-fold-work/>\n",
    "\n",
    "This is known as *k*-Fold Cross Validation.\n",
    "\n",
    "Note the following:\n",
    "\n",
    "- Every sample is eventually tested against\n",
    "- Similarly, you use all your data for training. In a scenario with limited data, this is an advantage.\n",
    "- In a *k*-fold you wil train *k* models and get *k* metrics\n",
    "\n",
    "Generally, after performing the experiment *k* times, you would report the results of all *k* models, noting if there is large variance between the the *k* different models.\n",
    "\n",
    "What are some of the disadvantages of cross validation?\n",
    "\n",
    "- Time consuming, as you need to train *k* models\n",
    "- Not possible for very large datasets: for example, for image analysis with very deep networks, training can take 1 week. 10 fold cross validation is not feasible in this scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c065f2-2fa1-4da6-8e03-a748132d61bb",
   "metadata": {},
   "source": [
    "## 5-Fold Cross Validation Example\n",
    "\n",
    "Here we will perform 10 fold cross validation on the a small dataset known as the Iris dataset.\n",
    "\n",
    "To do this we will use Sci-Kit Learn's `cross_val_score()` function, which handles all of the train/test splitting, the training, and simply returns the scores of all 5 folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d403e792-71a6-4953-afe6-5e0ce141d0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "model = svm.SVC(kernel='linear', C=1, random_state=42)\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "for fold, score in enumerate(scores):\n",
    "    print(f\"Fold: {fold+1}: score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d6edc2-1712-4f47-8799-5aa045c41061",
   "metadata": {},
   "source": [
    "What you see here are the scores for 5 different models. Each trained on a different train/test split.\n",
    "\n",
    "What you should look out for is that each of the models performs more or less the same, meaning your training strategy is not sensitive to the test data that it gets, meaning it should be generalisable when confronted with real world data.\n",
    "\n",
    "Now we will discuss how to choose the machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a75ac0-c8c2-407c-8109-93fa619332ef",
   "metadata": {},
   "source": [
    "---\n",
    "# Choosing a Machine Learning Algorithm\n",
    "\n",
    "Now that we have no issue getting and generating data, and we have an overview about how we might evaluate a model, let's see how you actually choose the algorithm for your particular task.\n",
    "\n",
    "Sci-Kit Learn is a comprehensive package, and contains many dozens of algorithms for training models. \n",
    "\n",
    "As we said previously, we mainly discuss supervised learning in this course, however the API used for training models in Sci-Kit Learn is basically the same for most of the algorithms.\n",
    "\n",
    "Because there are so many algorithms, you might wonder, how do I even choose an algorithm for my particular task. The following flowchart provides an overview:\n",
    "\n",
    "![Sci-Kit Learn Flowchart](./img/scikit-flowchart.png)\n",
    "\n",
    "This chart is actually interactive on the Sci-Kit Learn website, and clicking on each of the nodes brings you to the documentation for each of the particular algorithms. See the 'cheat sheet' here: <https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html> \n",
    "\n",
    "Because the focus of this course is not really about the algorithms per se, we will discuss the API that Sci-Kit Learn uses for training algorithms in general. \n",
    "\n",
    "These will differ from algorithm type to type, so for example clustering algorithms are unsupervised, and therefore do not accept a label parameter. These differences are clear from the documentation.\n",
    "\n",
    "As a first step, let's take classification as an example so that we can learn about the general look and feel of the API. In this case will take a look at the Suport Vector Machine classifier. \n",
    "\n",
    "Explaining how SVMs work is out of the scope of this seminar, but what we can say is that it is very widely used algorithm that performs well on a variety of tasks. It can be used for both linear and non-linear data. It can be used for both classification and regression tasks. And it can be used for both binary and multi-class classification.\n",
    "\n",
    "The general procedure for training any algorithm in SciKit Learn is to initialise an object of the algorithm in question, which will contain all the parameters that are specific to that algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cd929d-b6c4-4b64-ad0b-85b36d5084d4",
   "metadata": {},
   "source": [
    "---\n",
    "# Model Persistence \n",
    "\n",
    "Once you have a trained model, often you will want to deploy this model or use it some upstream task, or just save it to disk to be opened later, as perhaps it took many hours to train. \n",
    "\n",
    "In order to save models, SciKit Learn uses Python's pickle format. This is a serialised binary format. This means that, for example, you can serialise any Python object and save it to disk. Therefore to save a trained model, we just use default Python tools, which SciKit-Learn uses internally anyway. \n",
    "\n",
    "We have a model above that we trained for the 5-fold Cross Validation example, stored in the variable `model`. We can save this to disk easily, using Pyton's `pickle` module.\n",
    "\n",
    "This is done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc7c4d7-ff99-4f9d-b336-4ff88dbb690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Train our model above on the entire training set\n",
    "model.fit(X, y)\n",
    "\n",
    "pickle.dump(model, open('model.pickle', 'wb'))  # wb = write as binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf14344-def0-4d4a-8774-c40120566f94",
   "metadata": {},
   "source": [
    "The model has now been saved to the file `model.pickle`.\n",
    "\n",
    "This `model.pickle` can now be transferred elsewhere, sent to another party, stored for later use, or just opened in some other application's workflow. We will see this tomorrow, when we will integrate a model in to a web application.\n",
    "\n",
    "To demonstrate the reading of a model from disk, we can read the model file we just saved, and make it predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a7f76c-a26b-4515-b9a0-7ecf092ac81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_from_file = pickle.load(open('model.pickle', 'rb'))  # read as binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fca62d7-ba26-4959-94f6-c652233da8fe",
   "metadata": {},
   "source": [
    "Once opened, we can immediately use it to make some predictions on some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34142c6d-0cf0-4ac9-9396-b636424bbb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_from_file.predict(X[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f406488-7fa1-4b06-a9b8-98bbe06b3fc8",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Now we will perform an end-to-end machine learning classification. \n",
    "\n",
    "In this section, you will run the code, and answer some questions at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5a0f13-9094-43fe-8b99-eb133d888b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.tree import export_text\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51278de6-957b-4b91-8520-1d389c6a3c03",
   "metadata": {},
   "source": [
    "## Skin Lesion Classification \n",
    "\n",
    "In this section we will go in to a deeper dive to random forests and perform an end-to-end machine learning pipeline, using some of the methods we have learned above. We will then see how to analyse the performance of a model. Your exercise for this section will be to train a random forest model, and after this we will analyse the results and performance of the model together. \n",
    "\n",
    "Before you train the model, we will first gather a dataset, related to skin lesions. We first look at the dataset's overall structure, the types of data it contains, and we will see how we need to prepare the data, or clean the data. \n",
    "\n",
    "The dataset realtes to the classification of images **erythemato-squamous diseases** (**ESDs**). We will not analyse the images directly - we will use a dataset containing certain observations or characteristics for 366 skin lesions from patients. The characteristics are visual features that were recorded by a dermatologist. These include, for example, \"itching\", \"scaling\", or \"erythema\", as well as a severity score. \n",
    "\n",
    "- We can use these characteristics to tain a decision tree, rather than analysing the images directly. \n",
    "- This is an alternative to the deep neural networks (convolutional neural networks) commonly used today. To analyse images directly, typically you would use Deep Learning and Neural Networks. We will look at Deep Learning tomorrow.  \n",
    "- A decision tree uses the characteristics to learn **rules** for diagnosing an image\n",
    "\n",
    "Random Forests are often preferred in medicine as they are interpretable. You can view the rules that the trees use to make their decisions.\n",
    "\n",
    "So, once we have prepared the data for training, your task will be to train a random forest classifier. \n",
    "\n",
    "Here are some examples of the skin lesions, these are images of psoriasis however the dataset consists of a number of different skin diseases:\n",
    "\n",
    "<img src=\"./img/psoriasis-edit.jpg\" width=\"1200px\"/>\n",
    "\n",
    "*Image couresty of Dash, M et al. \"A cascaded deep convolution neural network based CADx system for psoriasis lesion segmentation and severity assessment\". Applied Soft Computing, 91 (2020).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7dc8a9-bc9b-4a98-8cca-d420a006f383",
   "metadata": {},
   "source": [
    "What are erythemato-squamous diseases (ESDs)?\n",
    "\n",
    ">*Erythemato-squamous diseases are common skin diseases. They consist of six different categories: psoriasis, seboreic dermatitis, lichen planus, pityriasis rosea, chronic dermatitis and pityriasis rubra pilaris. They all share the clinical features of erythema and scaling with very little differences. Their automatic detection is a challenging problem as they have overlapping signs and symptoms.*\n",
    ">\n",
    ">A. M. Elsayad, M. Al-Dhaifallah and A. M. Nassef, \"Analysis and Diagnosis of Erythemato-Squamous Diseases Using CHAID Decision Trees\". 15th International Multi-Conference on Systems, Signals & Devices (2018):252–262.\n",
    "\n",
    "There are 6 different types of erythemato-squamous diseases. They have very similar clinical features, and can have overlapping signs and symptoms. Therefore their detection is a challenging problem. Another difficulty in differential diagnosis is that a disease in its early stages may show the characteristics of another disease and only shows its characteristic features in later stages.\n",
    "\n",
    "Now that we know a little bit about our dataset, the first step is to load the dataset, and preview the first 10 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1163ddeb-1769-4ba0-8ec2-fea708414cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "derma = pd.read_csv('data/dermatology-clinical-only.tsv', sep='\\t')\n",
    "derma.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b7def8-6c5c-44ae-ad11-b46ad76d7b2f",
   "metadata": {},
   "source": [
    "This dataset is much cleaner and better prepared than the heart disease dataset we looked at previously. We can more or less leave it as it is, as most of the columns are already ordinally encoded. \n",
    "\n",
    "The only exception to this is the age, which we could scale using a scaler, as we did with the heart disease data. However, in this example, we will not do this for the sake of brevity.\n",
    "\n",
    "The first thing we might want to do is list the dataset's features. We can do this by looking at the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3129454b-566e-4c60-bdef-9ffe011e1410",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(derma.columns), columns=[\"Feature\"], index=range(1, (len(derma.columns)+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678bfbb2-bacb-42f4-a2db-c00805ef5122",
   "metadata": {},
   "source": [
    "Here you can see we have 12 features, as well as our diagnosis. \n",
    "\n",
    "Each feature is described in the paper relating to the dataset (Güvenir, H. Altay et al. \"Learning differential diagnosis of erythemato-squamous diseases using voting feature intervals\". Artificial Intelligence in Medicine, 13 3 (1998): 147-65), and can be seen below:\n",
    "\n",
    "| Clinical Attribute | Value Range | Description |\n",
    "|-------------------|-------------|-------------|\n",
    "| Erythema | 0-3 | Redness of the skin due to inflammation |\n",
    "| Scaling | 0-3 | Peeling or flaking of the outer skin layer |\n",
    "| Definite Borders | 0-3 | Clarity/distinctness of lesion boundaries |\n",
    "| Itching | 0-3 | Severity of pruritus (itching sensation) |\n",
    "| Koebner Phenomenon | 0-3 | Development of lesions at sites of skin trauma |\n",
    "| Polygonal Papules | 0-3 | Small, angular raised bumps on the skin |\n",
    "| Follicular Papules | 0-3 | Small bumps centered around hair follicles |\n",
    "| Oral Mucosal Involvement | 0-3 | Extent of mouth lining affected |\n",
    "| Knee and Elbow Involvement | 0-3 | Presence of symptoms at these joints |\n",
    "| Scalp Involvement | 0-3 | Extent of scalp affected |\n",
    "| Family History | 0-1 | Presence (1) or absence (0) of disease in family |\n",
    "| Age | Linear | Patient's age in years |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebef8bc-0f79-4dd5-b697-b91e21cbfb7f",
   "metadata": {},
   "source": [
    "We can see that a scoring system is used for most of the features. The scoring system uses the following scale for most features:\n",
    "\n",
    "- 0: Feature is absent\n",
    "- 1: Mild presence\n",
    "- 2: Moderate presence\n",
    "- 3: Maximum presence\n",
    "\n",
    "There are two exceptions:\n",
    "\n",
    "Family History:\n",
    "- 0: No family members have had any of these diseases\n",
    "- 1: At least one family member has had one of these diseases\n",
    "\n",
    "Age:\n",
    "- Recorded as the actual numerical age of the patient in years\n",
    "\n",
    "Note that the clinical features, such as Scaling or Scalp Involvement, are already ordinal and encoded properly.\n",
    "\n",
    "Let's now take a look at the diagnosis, to see how many different classes there are: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a41c88-b440-4ab8-9d80-4cb51e1d6c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(derma['diagnosis'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d6e7c9-7190-4dde-aaf8-4c37d2aab70e",
   "metadata": {},
   "source": [
    "So we can see that we are not even distributed. This can sometimes lead to issues with intepreting classification accuracy. We will learn more about this tomorrow.\n",
    "\n",
    "We can even plot this quickly, to get a better idea of the distrubtion of the target classes. Taking a look at the distribution of the target classes, is normally one of the first things you will look at if you given a dataset to analyse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96fa96c-9000-484c-ad1f-ab08cdf600b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "derma['diagnosis'].value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdea4c4-43bc-44eb-be77-b84bd6d01069",
   "metadata": {},
   "source": [
    "As mentioned, these are not evenly distributed. We will need to be careful how we interpret our results later..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b2305e-7033-4248-a16c-88ad5bd5c8c2",
   "metadata": {},
   "source": [
    "### Cleaning the Data\n",
    "\n",
    "After examining the class target distribution, and taking a look at how the dataset is structured then normally the next thing to do is prepare the data and 'clean' it. \n",
    "\n",
    "Cleaning data can consist of several things: \n",
    "\n",
    "- Examing the data types involved, for example some columns may be continuous such as age, some might be categorical/nominal (such as city), some might be ordinal (such as severity)\n",
    "- many algorithms need to have categorical data converted into numerical data, for example\n",
    "- handling missing data: either by removing rows with missing data, or imputing values using the mean or median (e.g. a missing age could be replaced with the mean age of the age column).\n",
    "- scaling the data, often between `0` and `1` or `-1` and `1`: this is to ensure features are treated equally - e.g. age might be between 0-100 while salary might go from 0-1,000,000 and larger scale features can end up dominating the learning process. Scaling means all values peak at 1, and the relative differences between samples is kept.\n",
    "- standardising data, for example dates might be inconsistently formatted (month/day vs. day/month) for example\n",
    "- standarding might also include ensuring units are stored in a consistent way, so that volume is stored consistently in ml, and any values like 1.3l is converted to 1300ml.\n",
    "- High cardinality: you may want to group certain categories. For example, you might have 10 rare diseases that appear only a few times each in the dataset, it might make sense to simply create one group called 'other' for all 10 of these diseases. Or similar categories can be merged, even if they appear often in the dataset.\n",
    "- Merging can also be used to balance the dataset, for example if you had 100 high grade tumour cases, and 40 intermediate grade tumour cases and 20 low grade tumour cases, you might just balance the dataset and create two classes: 100 high grade tumours and 60 intermediate/low grade tumours.\n",
    "\n",
    "Bear in mind, when converting data that we have these main types of categorical data:\n",
    "\n",
    "- **Ordinal**: the order has meaning. For example, the symptom severity above. We want to give the more severe symptom the largest value, and the values for the various other gradings are in relative order.\n",
    "- For example: severe is 3, moderate is 2, mild is 1, and no symptoms is 0.\n",
    "- On the other hand, **nominal** data has no order. For example, colours are not ordered in any way. Therefore, if you were to encode colours numerically, e.g. Blue is 4 and Red is 17, the algorithm might conclude that Red is 'better'/'stronger'/'worth more' than Blue - which is not the case. Therefore, nominal data is often **one hot encoded**\n",
    "\n",
    "Original dataset has a field for city, that we must encode for the algorithm to be able to handle it. Most algorithms will not work with strings, they need to be encoded numerically some how. Here is the original dataset:\n",
    "\n",
    "| Age | City   | Diagnosis |\n",
    "|-----|--------|-----------|\n",
    "| 77  | London | Malignant |\n",
    "| 44  | NYC    | Benign    |\n",
    "| 24  | Vienna | Benign    |\n",
    "\n",
    "You wish to encode the City feature, however this is **nominal** and we do not wish to imply some sort of order (even if this existed, we might not know the order). \n",
    "\n",
    "Therefore, the values are encoded as vectors: istead of `London = 1`, `NYC = 2`, and `Vienna = 3` we have `London = [1, 0, 0]`, `NYC = [0, 1, 0]` and `Vienna = [0, 0, 1]`. The `1` merely reprsents the presence or absense of a value. The dataset would then look like this:\n",
    "\n",
    "| Age | London | NYC | Vienna | Diagnosis |\n",
    "|-----|--------|-----|--------|-----------|\n",
    "| 77  | 1      | 0   | 0      | Malignant |\n",
    "| 44  | 0      | 1   | 0      | Benign    |\n",
    "| 24  | 0      | 0   | 1      | Benign    |\n",
    "\n",
    "Now we have 3 features, and the presense of a 1 says that this person lives in this city. It says nothing else about the other cities, only that the patient does **not** live there. \n",
    "\n",
    "It is also good practice to document any changes you make to the data, as you will normally need to make certain assumptions as you clean the data. Jupyter notebooks serve as a perfect way to document the changes as you can display step by step exactly what you did to the data in order to clean it.\n",
    "\n",
    "With this dataset, it is relatively easy because all the cinical fields have values between 0-3 and are already ordinal and require no further editing work. The exceptions are the family history which is binary (0 or 1), and age which is continuous. The binary field also does not require work. The age field could be scaled, if required, but we will not do this in order to save time.\n",
    "\n",
    "Therefore, the only cleaning we really have to do is for missing values and remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fc12ac-a546-4fe6-83e8-990510d498a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features to numeric\n",
    "for col in derma.columns:\n",
    "    if col != 'diagnosis':\n",
    "        derma[col] = pd.to_numeric(derma[col], errors='coerce')\n",
    "\n",
    "# Handle missing ages (replace with median)\n",
    "derma['age'] = derma['age'].replace('?', np.nan)\n",
    "derma['age'] = pd.to_numeric(derma['age'])\n",
    "derma['age'] = derma['age'].fillna(derma['age'].median())\n",
    "\n",
    "# Finally, if we missed any missing data we can just drop the rows that contain missing values\n",
    "derma = derma.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cec766-513b-4441-8b0a-cff072f70f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "derma.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191b111e-12ef-48ad-88bb-c0b9662c40a8",
   "metadata": {},
   "source": [
    "Once this is done, we will train a Random Forest model.\n",
    "\n",
    "Random Forests are a powerful and often used model in medicine due to their ability to handle complex data while also maintaining interpretability. This contrasts to many other models which are 'black boxes' such as neural networks, where it is very difficult to work out why they have made a particular classificaiton. In healthcare applications, Random Forests's inherent feature importance ranking helps clinicians identify which variables most strongly influence predictions, making them valuable for both diagnostic and prognostic modeling. Furthermore, Random Forests are relatively resistant to overfitting compared to other complex models, making them particularly suitable for medical datasets that are often limited in size.\n",
    "\n",
    "Run the code to train the Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97da4fed-08f1-4222-ad2e-1f9be52d70d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = derma.drop('diagnosis', axis=1)\n",
    "y = derma['diagnosis']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505d96b7-1e6f-41f1-b682-b137272f63f4",
   "metadata": {},
   "source": [
    "We do not perform a 5-fold cross validation in order to keep things simple. \n",
    "\n",
    "Now that we have trained the model, let's look at the overall accuracy of the model at classifying the lesions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9886618f-48f3-4136-ae84-af7a8b1cd0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {(accuracy * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aca41f-1594-47bd-8fd4-9fc779c715f3",
   "metadata": {},
   "source": [
    "This is the accuracy across all 6 skin lesion classes, and looks like a good score.\n",
    "\n",
    "However... relying on only this metric might be problematic for a few reasons, namely we have a unevenly distributed diagnosis class. \n",
    "\n",
    "Let's look at the distribution of the classes in the test set. This can be done using the `classification_report()` function, which breaks down the accuracy on a class-by-class basis, and also provides other valuable information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa3cea1-2dc9-4445-b67f-084499ee9b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a37169-f225-409e-9d90-cbc02c8250a0",
   "metadata": {},
   "source": [
    "What can be observed here?\n",
    "\n",
    "First of all, the `support` field shows how many samples from each class are in the test set. Observe that this is not balanced.\n",
    "\n",
    "For example, notice that psoriasis is over-represented. There are 31 samples out of 74. This is 42% of all the data. Now imagine the network classified all lesions as psoriasis, then it would be 42% accurateo overall... Why might this be a problem?\n",
    "\n",
    "Also observe that Pityriasis Rubra Pilaris has perfect accuracy, however there are only 3 samples so we need to be careful with how we interpret this. \n",
    "\n",
    "A few other observations, from a clincal point of view:\n",
    "\n",
    "- For less common conditions like Pityriasis Rubra Pilaris, the sample size is quite small (only 3 cases), so the perfect score should be interpreted cautiously\n",
    "- The model seems to be most reliable at ruling in Lichen Planus and Psoriasis (high precision)\n",
    "- It's best at not missing cases of Pityriasis Rosea (perfect recall) but may overdiagnose it\n",
    "- The model has an overall accuracy of 85%, which is quite good for a multi-class skin condition classifier\n",
    "\n",
    "Hence, you must carefully analyse the results of a model, printing the accuracy overall is often not enough. \n",
    "\n",
    "Another good way to analyse and evaluate a model, is through the use of a so-called confusion matrix. \n",
    "\n",
    "SciKit-Learn includes a `confusion_matrix()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87374d42-3795-44e3-b964-fc95296e98c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a1475c-9bda-4adb-ba54-3a2e97387cda",
   "metadata": {},
   "source": [
    "On its own, this might not look very useful. However if you plot it, with some colour in the form of a heatmap, it is a very useful tool to interpret and evaluate a model's classification performance.\n",
    "\n",
    "Let's plot the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c60795-0220-4943-91ce-4f098d75b4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the confusion matrix\n",
    "classes = sorted(y.unique())\n",
    "\n",
    "cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues',\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Confusion Matrix for Dermatological Diagnoses', fontsize=14, pad=20)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "\n",
    "# Rotate x-labels for better readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "# Adjust layout to prevent label cutoff\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bf7732-b79e-4d48-ad58-220d341657ae",
   "metadata": {},
   "source": [
    "The confusion matrix shows the model's \"confusion\" at predicting each class.\n",
    "\n",
    "It plots the true labels versus the predicted labels for each class, and shows how often the predictions were correct, and when incorrent, what were the incorrect predictions.\n",
    "\n",
    "For example, we can see that psoriasis was correctly predicted 26 times, but incorrectly predicted as pityriasis rosea twice, and seborrheic dermatitis 3 times.\n",
    "\n",
    "Sometimes it makes sense to normalise the confusion matrix, so that the heatmap is more obvious:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bd2e28-ee4f-449c-aa44-94e7823a7e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the confusion matrix\n",
    "classes = sorted(y.unique())\n",
    "\n",
    "cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Create heatmap\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Reds', xticklabels=classes, yticklabels=classes,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Confusion Matrix for Dermatological Diagnoses (Normalised)', fontsize=14, pad=20)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "\n",
    "# Rotate x-labels for better readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "# Adjust layout to prevent label cutoff\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a86444d-605d-48bc-83cf-82a28fea46d1",
   "metadata": {},
   "source": [
    "This shows the arruracy of the model more clearly, in percentage terms.\n",
    "\n",
    "At a quick glance, the darker the diagonal, the better the model's classification accuracy across all classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69c4533-80f1-445e-bda8-131d060588d7",
   "metadata": {},
   "source": [
    "## Interpreting the Model\n",
    "\n",
    "In this section we will now try to interpret the Random Forest **model** itself, rather than the results of the model.\n",
    "\n",
    "### Visualise the Decision Tree\n",
    "\n",
    "Radom Forests, as their name suggests, consists of many Decision Trees, and these trees are rule based classifiers. \n",
    "\n",
    "The trees learn yes-no rules based on the data, and these rules can be interpreted by visualising the trees. \n",
    "\n",
    "A Random Forest can contain many hundreds or thousands of decision trees. When a Random Forst makes a classification, each tree makes a classification indepently, and a vote occurs as to what the overall classification is. \n",
    "\n",
    "Below we will visualise a tree by plotting it. Our model consisted of 100 trees, we will visualise tree `0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dc1788-f7b7-41d2-8506-b76acbe004d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first tree in the forest\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(rf.estimators_[0], \n",
    "          feature_names=X.columns,\n",
    "          class_names=classes,\n",
    "          filled=True,\n",
    "          max_depth=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10dee09-26ce-41de-847b-a40717aabbb8",
   "metadata": {},
   "source": [
    "As you can see, the rules are basically if-else statements. The top node asks `IF scalp involvement <= 0.5 THEN LEFT ELSE RIGHT` for example.\n",
    "\n",
    "Data is passed through the tree until it reaches an end node, and this is the classification result.\n",
    "\n",
    "The tree above is too large to be printed to screen, so therefore it is often easier to print the tree as text. \n",
    "\n",
    "We do so below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30043675-f4c9-46c0-be77-4b3175abc8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_text = export_text(rf.estimators_[0], \n",
    "              feature_names=X.columns,\n",
    "              class_names=classes)\n",
    "\n",
    "print(tree_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c4f537-3235-42e8-b48f-9483ddcfdbd7",
   "metadata": {},
   "source": [
    "Here all the rules are visible and nothing is trucated. \n",
    "\n",
    "You can use these tree visualisations to understand the rules that the Random Forest's trees are making when performing a classification, making them somewhat interpretable. \n",
    "\n",
    "Because we have access to the rules that each tree contains, feature importances can also be generated from Random Forest models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056ad8ca-e013-4031-a099-dd1b54e0b638",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "As we just mentioned, we can also analyse which **features contrbuted the most to the classifications**. \n",
    "\n",
    "This might give us useful clinical insight into Erythemato-squamous diseases. For example, if the model consistently showed that knee and elbow involvement was an important feature in diagnosing Erythemato-squamous diseases, and this was not known before, it might provide new insights into the disease that could be further examined.\n",
    "\n",
    "Run this code to see the top feature importances (they are available in our Random Forest model, `rf` in the `feature_importances_` attribute):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e10c785-e5cf-4508-b89b-950422c16328",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFeature Importance:\")\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2843c25a-54f6-4f45-bbb8-60b5c2d283d7",
   "metadata": {},
   "source": [
    "We can also plot this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611cac62-9e52-4464-bf2b-a05346d03bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=importances)\n",
    "plt.title('Feature Importance in Random Forest Model')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22872ce7-7d24-4d77-862c-bbb942335b53",
   "metadata": {},
   "source": [
    "### Grid Search \n",
    "\n",
    "You may have noticed that there are quite a large number of parameters that can be chosen for the random forest algorithm. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1fed0b-3936-4b78-ad50-302b6e530997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid as a dictionary of parameters \n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'max_samples': [0.5, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Calculate total number of combinations\n",
    "n_combinations = np.prod([len(v) for v in param_grid.values()])\n",
    "print(f\"Total number of combinations to try: {n_combinations}\")\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4165b58-fd66-4e74-b378-70538c1b9101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of all results\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Sort results by mean test score\n",
    "results_sorted = results.sort_values('mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c967e8c8-7f2e-41e7-b4e6-b532a6df9454",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e386b13d-76e2-484c-96f7-3f145d6ce19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTop 5 parameter combinations:\")\n",
    "cols_to_show = ['params', 'mean_test_score', 'std_test_score']\n",
    "results_sorted[cols_to_show].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c145e6b7-651f-4e78-a0a1-5d9348b3857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(results['mean_test_score'], bins=20)\n",
    "plt.title('Distribution of Cross-Validation Scores')\n",
    "plt.xlabel('Mean Test Score')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38242e89-117a-4acb-a7c1-3e8d39b6dad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curves(estimator, X, y):\n",
    "    train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=5, n_jobs=-1, train_sizes=train_sizes,\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    \n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_sizes, train_mean, label='Training score')\n",
    "    plt.plot(train_sizes, test_mean, label='Cross-validation score')\n",
    "    \n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)\n",
    "    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1)\n",
    "    \n",
    "    plt.xlabel('Training Examples')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Learning Curves for Best Model')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6fd0db-f44c-4bf9-add1-6181a0bdaefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575e5fa2-e293-41f5-b725-bbcd9d7cf013",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882c431f-ae25-456a-8b18-7fb11dc674b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(best_model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcc3e88-5d9d-431a-b10a-076be7c2036a",
   "metadata": {},
   "source": [
    "What does this show? From the SciKit Learn documentation:\n",
    "\n",
    "- This can show you if getting more data will help you: if both lines level out and plateau then adding more data probably won't help.\n",
    "- Conversely, if the lines do not converge, then you probably would benefit from more data.\n",
    "- If both lines plateau early on (i.e. at a low performance level), then you might be **underfitting**: try a model with more complexity. In terms of Random Forests that means more depth, or more trees.\n",
    "- If there is a large gap between the training score and the test score (i.e. the training score is always much better) then you are **overfitting**. More data is often a solution to overfitting. Regularisation is another method - this prevents models from becoming too complex, which disallows it from learning the data exactly. This is different depending on the algorithm. In Random Forests you might want to control the maximum depth of the trees, for example.\n",
    "\n",
    "Ideally you want both curves high and close together and to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fc572b-ffb9-448a-ad0f-890303044ad0",
   "metadata": {},
   "source": [
    "Parameter importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa33d132-dc16-41e7-b3c6-3a99526916a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_importance = pd.DataFrame({\n",
    "    'Parameter': [],\n",
    "    'Impact': []\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8bb13a-89df-4c38-8f20-0a227b1d1439",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in param_grid.keys():\n",
    "    scores = []\n",
    "    values = param_grid[param]\n",
    "    for value in values:\n",
    "        mask = results['params'].apply(lambda x: x[param] == value)\n",
    "        score = results[mask]['mean_test_score'].mean()\n",
    "        scores.append(score)\n",
    "    impact = max(scores) - min(scores)\n",
    "    param_importance = pd.concat([param_importance, \n",
    "                                  pd.DataFrame({'Parameter': [param], 'Impact': [impact]})],\n",
    "                                  ignore_index=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(param_importance['Parameter'], param_importance['Impact'])\n",
    "plt.title('Parameter Impact on Model Performance')\n",
    "plt.xlabel('Parameter')\n",
    "plt.ylabel('Impact (Max - Min Score)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2566d4-f285-4d39-b70d-9d2d3bc03f2e",
   "metadata": {},
   "source": [
    "## End of Day 1\n",
    "\n",
    "That was our content for Day 1.\n",
    "\n",
    "Day 2 will focus on Neural Networks and Deep Learning, as well as model deployment.\n",
    "\n",
    "Topics include:\n",
    "\n",
    "- Simple neural networks: we will use PyTorch to define a simple neural network and train it\n",
    "- PyTorch: we will discuss the framework we will use to create neural networks\n",
    "- Deep learning: we move to deep networks, the basis for all of the recent advancements, such as generative models and GPT models\n",
    "- Image classification: we will train an image classifier on a number of small tasks\n",
    "- Image segmentation: we discuss image segmentation in the context of medicine\n",
    "- Pre-trained models: use networks that have already been trained\n",
    "- Fine-tuning models: adapt a pre-trained network to your specific task \n",
    "- Model deployment and web application development\n",
    "- Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d19f71-c5ce-42f8-a9e8-e03d9ece5d80",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
