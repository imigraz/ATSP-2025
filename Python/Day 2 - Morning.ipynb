{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a07488-7854-49c1-8262-661921321626",
   "metadata": {},
   "source": [
    "# Day 2 - Morning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca0bba3-d59f-4905-8f34-82ea764f4703",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5092a9c-46ee-4d6f-ad7e-de05dbe9cd4d",
   "metadata": {},
   "source": [
    "In this section of the course we will discuss **Deep Learning**, and how to apply deep learning to your data.\n",
    "\n",
    "Deep Learning is fundamentally different to 'classical' machine learning in a number of ways, which we will discuss in due course. \n",
    "\n",
    "These days, deep learning is used mostly in \n",
    "\n",
    "- image data\n",
    "    - image classification\n",
    "    - object detection (counting objects within images)\n",
    "    - image segmentation (lesions in MRI for example)\n",
    "- generative models, for example LLMs like ChatGPT\n",
    "\n",
    "In Deep Learning, in contrast to what we saw yesterday, you are normally performing much feature engineering. We generally do not select features or perform feature engineering - we in fact allow the network to learn which features are important all by itself. \n",
    "\n",
    "Normally, in the problems we discussed yesterday, we carefully selected features, removed features, or acquired datasets where features were curated in order for us to be able to train a model that works well. In deep learning, we often leave this to the network itself to work out. We \n",
    "\n",
    "I will illustrate this with a demo:\n",
    "\n",
    "<https://playground.tensorflow.org>\n",
    "\n",
    "In the playground, we will\n",
    "\n",
    "1. Mimick feature engineering (all except $\\sin$) and train a network using these features and 8 neurons on the swiss roll data\n",
    "2. Remove all engineered features and instead use multiple layers, achieving the same thing (ensure the use of ReLU activation function)\n",
    "\n",
    "As we saw, adding layers upon layers allows the network to learn very complex functions. \n",
    "\n",
    "We will look at an example of MNIST (Modified National Institute of Standards and Technology) dataset demonstrates this very well. \n",
    "\n",
    "Imagine you had a house price dataset. Well the features are quite easy to understand, and you could probably easily remove features that were not important, such as house colour, or street number. \n",
    "\n",
    "This also means there is far less domain knowledge required to train neural networks. You might not neccessarily need to know the details of your data in order to train something - for example in medicine, you might not have to know the details of the RNA sequence data that you wish to classify. \n",
    "\n",
    "## Why *Deep* Learning?\n",
    "\n",
    "![simple-network](./img/net-single-layer.png)\n",
    "\n",
    "*Source:* <http://neuralnetworksanddeeplearning.com>\n",
    "\n",
    "So, neural networks have been around for decades, and for many years were actually ignored. It is only recently that we have seen a resurgance of interest in neural networks and specifically the rise in **deep** learning.\n",
    "\n",
    "This has happened for 2 main reasons\n",
    "\n",
    "1. We have become very good at gathering lots of data\n",
    "2. Hardware has become very powerful at training neural networks\n",
    "\n",
    "This has led to networks getting larger and larger, meaning they can perform ever more complex tasks. In many fields, such as image classificaiton, they are in a class of their own in terms of capabilities, as is the case with machine translation, image generation, image segmentation, and so on.\n",
    "\n",
    "![deep-network](./img/net-deep-layer.png)\n",
    "\n",
    "*Source:* <http://neuralnetworksanddeeplearning.com>\n",
    "\n",
    "This brings up a further point: hardware. Neural networks require lots of computing power to train, and it turns out that the devices that are best suited for performing the calculations required to train a neural network are in fact GPUs (graphics processing units) and not CPUs. Most computers, especially laptops, do not have a dedicated GPU that is powerful enough to train neural networks. However, it is still possible to train networks using a PC's CPU, it is magnitudes slower.\n",
    "\n",
    "However, for smaller datasets, as we will see here, training can be performed on a CPU. For example training a small network that has to classify small images can be trained on a normal PC or laptop. \n",
    "\n",
    "Second, there are ways to get a GPU for free to perform training tasks. For example, Google Colab offers GPUs for free, and I will demo this later.\n",
    "\n",
    "![feature-hierarchy](./img/feature-hierarchy.png)\n",
    "\n",
    "Image source: Raphael, Dubinsky, Iluz, Netanyahu, (2020). Neural Network Recognition of Marine Benthos and Corals. Diversity. 12 (29).\n",
    "\n",
    "### Areas Where Deep Learning is Used\n",
    "\n",
    "Neural Networks and Deep Learning algorithms can be applied to nearly any type of problem, but they are more suited to some problems than others. For example, they tend not to be used for tabular/text data, such as those we discussed yesterday, where algorithms like random forests can still outperform them. See for example the paper: Grinsztajn L, Oyallon E, Varoquaux G. Why do tree-based models still outperform deep learning on tabular data? arXiv: <http://arxiv.org/abs/2207.08815>\n",
    "\n",
    "So in this section we will concentrate on deep learning for image analysis, and specifically image classification. In medicine, deep learning is mostly used for image classification or image segmentation. In this seminar, we will demo image classification, but we will also discuss how segmentation is performed.  \n",
    "\n",
    "Here are some of the fields and areas where deep learning is very successful\n",
    "\n",
    "- NLP: In natural language processing (NLP), networks have been trained at answering questions, speech recognition, document summarisation, document classification \n",
    "- Computer vision: Satellite imagery interpretation, face recognition, image captioning, reading traffic signs, highlighting and recognising pedestrians and vehicles in autonomous vehicles \n",
    "- Medicine: Finding anomalies in radiology images, including CT, MRI, and X-ray images; counting features in pathology slides; measuring features in ultrasounds; diagnosing diabetic retinopathy\n",
    "- Image segmentation: recent developments in networks such as U-Net have made deep neural networks state of the art at performing image segmentation, and is particularly used in medicine\n",
    "- Biology: protein folding (DeepMind), classifying proteins, genomics tasks, protein/protein interactions\n",
    "- Image generation: Colorising images, increasing image resolution (super resolution), de-noising/removing noise from images, converting images to art in the style of famous artists\n",
    "- Recommendation systems: product recommendations, film recommendations, music recommendations\n",
    "- Playing games: Chess, Go, Atari video games, and many real-time strategy games\n",
    "- Robotics: Boston Dyamics robots that can run, climb, and so on.\n",
    "\n",
    "(Some of the points above from: <https://course.fast.ai> which provides some very good introductory material on Deep Learning.)\n",
    "\n",
    "Also see this dataset: <https://www.robots.ox.ac.uk/~vgg/data/pets/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dc3e3f-bd85-4e99-bed7-0264c73ee883",
   "metadata": {},
   "source": [
    "## Funny Joke\n",
    "\n",
    "Deep learning has moved very fast in the past few years. Here is an example from the comic xkcd:\n",
    "\n",
    "<img src=\"./img/task.png\" width=\"400\"/>\n",
    "\n",
    "*Source*: <https://xkcd.com/1425/>\n",
    "\n",
    "This comic is not particulary old, and the text suggests that to build a system that can recognise birds would be 'virtually impossible', yet what we will see now is that in fact, we can perform a classification of this type very quickly, and we will do so during this course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71d2509-ef61-4c17-8358-5dddc9b6aa91",
   "metadata": {},
   "source": [
    "## How Neural Networks Work\n",
    "\n",
    "Here we will describe briefly the concepts behind neural networks. \n",
    "\n",
    "They are called neural networks because they are based on the idea of neurons and loosely mimick the neurons in the (human) brain.\n",
    "\n",
    "![neuron](./img/chapter7_neuron.png)\n",
    "\n",
    "*Source:* <https://colab.research.google.com/github/fastai/fastbook/blob/master/01_intro.ipynb>\n",
    "\n",
    "The neuron in neural network is based loosely on the biological neuron. In a neural network, we have a lot of interconnected neurons organised in layers, where neurons can fire or not fire based on their inputs and a weight. The weight associated with the neuron controls if it fires or not, and these weights are adjusted (or learned) during the training of the network. Ultimately, a network's ability to perform some function is based on the firing of these neurons. The neuron fires based on a threshold which is controlled by something called an activation function. By 'fires' we mean it outputs something (a value). The threshold is controlled using a weight. These weights are what are learned when the network is training. So what the network learns, is which neurons should fire and which should not.\n",
    "\n",
    "These weights are updated when you train the network. During training you pass data through the network, and the weights are updated based on the error or loss of the network. If the error is very high for a given prediction during training, these weights are nudged more than if the error was very small.\n",
    "\n",
    "Hopefully you now have some understanding how neural networks work, and how they are trained. Check the TensorFlow Playground website again if you wish to visually verify any of these concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329058f4-3223-48ac-b10f-75955b952e81",
   "metadata": {},
   "source": [
    "## Today's Topics\n",
    "\n",
    "**Morning**:\n",
    "- Deep Learning Frameworks\n",
    "- PyTorch\n",
    "- Training a simple network\n",
    "- Training a deep network\n",
    "- Medical Image Segmentation\n",
    "- Walkthrough of a Medical Image classification algorithm\n",
    "- Exercise 1\n",
    "\n",
    "**Afternoon**:\n",
    "- Deploying a deep learning model\n",
    "- Training with GPUs and Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab6ec6e-1946-4a7e-a447-b55cb3425f4d",
   "metadata": {},
   "source": [
    "---\n",
    "# PyTorch\n",
    "\n",
    "There are a number of frameworks available for neural network development:\n",
    "\n",
    "- PyTorch (which we will use during this course), from Facebook\n",
    "- TensorFlow, from Google\n",
    "- Keras (front-end for TensorFlow and PyTorch, higher level API)\n",
    "- FastAI (high level API, good for quick experimentation, and getting a model trained quickly and easily)\n",
    "\n",
    "As mentioned, we will be using Torch during this course. Torch is probably the most popular framework for deep learning currently. \n",
    "\n",
    "Unfortunately, traing neural networks require a lot of boilerplate code in order to get running. \n",
    "\n",
    "However, there are easier approaches and I will demonstrate these after we have covered how a network is trained generally.\n",
    "\n",
    "First some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4188520e-a965-4515-b5fd-9acd89d20707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import torchinfo\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe1306f-77c8-43ac-aa8a-51861b5dd275",
   "metadata": {},
   "source": [
    "## Classifying Hand Written Digits\n",
    "\n",
    "In this example, we are going to train a simple neural network to learn how to classify hand-written digits. \n",
    "\n",
    "MNIST is included in PyTorch, but there are many datasets to choose from, see the PyTorch documentation for details: <https://pytorch.org/vision/stable/datasets.html>\n",
    "\n",
    "The MNIST dataset consists for 60,000 training images and 10,000 testing images. They are only 28x28 pixels in size, meaning it is a very popular dataset for benchmarking and demonstrating neural networks.\n",
    "\n",
    "![MNIST](./img/MnistExamplesModified.png)\n",
    "\n",
    "By Suvanjanprasai - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=132282871\n",
    "\n",
    "So, the aim of this task is to train a neural network to recognise the digits. In other words, once trained, the network should be able to predict the digit contained in an image passed to the network.\n",
    "\n",
    "Learning to recognise digits is a **supervised** machine learning task: the vast majority of neural network based trained is supervised. Neural networks can be used for both classification and regression. In the case of MNIST, it is a supervised, multi-class classification.\n",
    "\n",
    "Training a neural network in PyTorch is a relatively complex task. However, later we will discuss much easier ways to do this.\n",
    "\n",
    "For now, however, we will demonstrate how this is done by first defining a neural network, training it over a number of **epochs** and then testing it on some unseen data. An epoch is one pass through \n",
    "\n",
    "The general procedure for training a neural network is as follows:\n",
    "\n",
    "1. Define your dataset, and any transformations you wish to perform on the data\n",
    "2. Define the neural network structure itself. This requires piecing together the layers and define the network's shape and structure, and so on\n",
    "3. Define the network's optimiser. You might remember from yesterday that algorithms in Sci-Kit Learn do not normally require you to define the optimisation algorithm. This is not true for PyTorch, as an optimiser must be defined explicitly.\n",
    "4. Define the network's loss: this depends on your problem, e.g. classification requires a certain type of loss function, etc. \n",
    "5. Train the network: you train it over a number of epochs that you decide on\n",
    "\n",
    "Note: this is the procedure when you want to train a network from scratch. There are easier options, and we will see these later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0cf57a-3af5-4563-a1d7-74e71dc2c5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Set download to true if the data is not already downloaded.\n",
    "train_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c1e2b4-8f0d-4139-bf70-2f7208961914",
   "metadata": {},
   "source": [
    "## Creating a Simple Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12157d50-fc29-4f32-9d75-643d31367caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the Neural Network Model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 500)  # Size must match the input\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()   \n",
    "\n",
    "torchinfo.summary(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f429544-9f72-4b9f-ae1a-c893e1418c12",
   "metadata": {},
   "source": [
    "## Define Loss and Optimizer\n",
    "\n",
    "Next we define our loss function and optimizer. The loss function is something which gives us a numeric value for how well or poorly the network has made a prediction. In other words,  it measures how far a neural network's predictions are from the correct answers. The higher the loss, the worse the prediction. \n",
    "\n",
    "An optimiser is the algorithm that updates and adjusts the neural network's weights to reduce this loss. It decides how to adjust these values to make the network produce a better prediction next time it sees the data, helping the network learn from its mistakes.\n",
    "\n",
    "We won't discuss the details of loss functions or optimisers in this course, only to say that for classification tasks Cross Entropy Loss is very commonly used, and common optimisers include SGD (Stochastic Gradient Descent) or Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b09c4f-28a4-42d3-bc03-b7665e3573d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350145ec-7802-47a8-b596-ee7482f11c27",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Now, we will train our network over 3 epochs. \n",
    "\n",
    "That means we will pass all our training data through the network 3 times in total.\n",
    "\n",
    "Execute this code to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c30beb-0de7-4e93-abf9-d206438aeb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_loader, optimizer, criterion, epochs=5):\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "train(net, train_loader, optimizer, criterion, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cf5d5d-3c84-45d1-9b53-16ba0ac651e2",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bc9807-2727-40fe-b191-7e224d5e2db4",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can evaluate it. \n",
    "\n",
    "This means, we will take our test set data, and ask the network to make predictions for each of the images in the test set.\n",
    "\n",
    "Then it will output an average accuracy, that is how many times the network was correct in its prediction.\n",
    "\n",
    "Run the following code to evaluate the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10bd8cb-4b1b-49cc-94b6-f5cb69326fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, test_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "accuracy = evaluate(net, test_loader)\n",
    "print(f\"Model accuracy: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85644ec5-9c12-4a4a-a7d5-662121d305ec",
   "metadata": {},
   "source": [
    "So we can see the network was correct over 95% in its predictions, given a new image from the test set.\n",
    "\n",
    "Later, we will see how to evaluate neural networks more carefully, including looking at the classification per class, and we will see how merely looking at the accuracy is not sufficient most of the time. However for now, we just want to demonstrate how to train a simple network. \n",
    "\n",
    "Now, we will show some predictions from the network. Running the following code will do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8856f2-df69-4c58-9262-21aea7318a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(test_loader)\n",
    "\n",
    "# Get the next batch from the iterator\n",
    "#images, labels = next(iterator)\n",
    "\n",
    "# Or loop over the first 1 batches: (use for images,labels in iterator: for all of them)\n",
    "for images,labels in itertools.islice(iterator, 1):\n",
    "\n",
    "    for image, label in zip(images, labels):\n",
    "        image_for_pred = image.unsqueeze(0)  # Add a batch dimension\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = net(image_for_pred)\n",
    "            predicted_label = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "        # Show false positives by saying 'is not; here\n",
    "        if predicted_label.item() is label.item():\n",
    "            plt.imshow(image.squeeze(), cmap='gray')\n",
    "            plt.title(f'Actual Label: {label}')\n",
    "            plt.show()\n",
    "            print(f'Predicted Label: {predicted_label.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eba564-542b-4089-8d2e-cdec63415848",
   "metadata": {},
   "source": [
    "# CIFAR10\n",
    "\n",
    "Let's move on to a more difficult example, where we have images of natural objects from 10 distinct classes. The images are also in colour. \n",
    "\n",
    "In this section we will take a look at more classification metrics, and how to better understand how well your network is performing.\n",
    "\n",
    "For this tutorial, we will use the CIFAR10 dataset. It has the classes: 'plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', and 'truck'. The images in CIFAR10 are of size 3x32x32, in other words 3 channel (RGB) images, 32x32 pixels in size.\n",
    "\n",
    "Here are some examples of the images contained in the CIFAR10 dataset:\n",
    "\n",
    "![CIFAR-Examples](./img/cifar10.png)\n",
    "\n",
    "Let's get the data, which we will get using Torch's `datasets` module, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421d4b11-b7d9-4eea-aad3-f3824c5ab59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchinfo\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# These are transformations applied to every image in the dataset\n",
    "# They can include image resizing, coverting to greyscale, and so on\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731398e1-b624-4936-bdce-1c6f1b1af264",
   "metadata": {},
   "source": [
    "One thing that always makes sense is to take a look at the distribution of the data, first with the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aacb677-972a-4d12-89e8-57ab91190880",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(trainset.targets, return_counts=True)\n",
    "pd.DataFrame(np.asarray((unique, counts)).T, columns=['class id', 'count'], index=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c09967-7f93-4ba1-95ed-9f3f4530a122",
   "metadata": {},
   "source": [
    "You can see that the classes are perfectly evenly distributed. This is by far not always the case. Often you will get datasets with a very wide class distribution. We will see examples of this later, and how this affects how we evaluate the network.\n",
    "\n",
    "Let's also take a look at the distrubtion of the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b7fcb5-cb99-4cf4-8225-af649633dd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(testset.targets, return_counts=True)\n",
    "pd.DataFrame(np.asarray((unique, counts)).T, columns=['class id', 'count'], index=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df140c1e-9e3c-4cef-a1e7-16925662ab72",
   "metadata": {},
   "source": [
    "We can preview some of the images from the dataset and also see their labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7d2790-60df-42b3-8719-dc8a5c5ec5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5 # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "data_iterator = iter(trainloader)\n",
    "images, labels = next(data_iterator)\n",
    "\n",
    "# show images\n",
    "imshow(make_grid(images))\n",
    "\n",
    "# print labels\n",
    "labels_t = [f'{classes[labels[x]]}' for x in range(batch_size)]\n",
    "print(labels_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100a24c9-88dd-43ba-ac1c-15b845dd6d12",
   "metadata": {},
   "source": [
    "Let's now define a network. \n",
    "\n",
    "This is a more complex network that the one above, as we will see when we print its structure and see how many parameters it has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20ce601-96b1-4549-be85-e5e1f039bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "torchinfo.summary(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f532f6-16a3-454a-bbae-4b5de7514ff0",
   "metadata": {},
   "source": [
    "Again, we define an optimiser and a loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f56d24-62aa-4486-9c80-9c451166bc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edfe532-4541-4830-8fb0-a4da5468916e",
   "metadata": {},
   "source": [
    "And train over 5 epochs (the data is passed through the network 5 times):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc894f0-b5dd-4d3d-bed7-5c6438ac251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(tqdm(trainloader), 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        #running_loss += loss.item()\n",
    "        #if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "        #    print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "        #    running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d197afa1-3e7b-41cb-94c3-7599053672a3",
   "metadata": {},
   "source": [
    "Now that we have finished training, we can test the network on the held out test set! \n",
    "\n",
    "First let's look at a batch from the test set, along with their labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f45fe-d635-4e93-bb3f-a59d041fcdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterator = iter(testloader)\n",
    "images, labels = next(data_iterator)\n",
    "\n",
    "# Create a grid of images\n",
    "grid_img = torchvision.utils.make_grid(images)\n",
    "\n",
    "# Denormalize: Convert from [-1,1] to [0,1] range\n",
    "grid_img = (grid_img + 1) / 2\n",
    "\n",
    "# Convert to the format matplotlib expects\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(grid_img.permute(1, 2, 0).cpu().numpy())\n",
    "plt.axis('off')\n",
    "plt.title('GroundTruth: ' + ' '.join(f'{classes[labels[j]]:5s}' for j in range(len(labels))))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0661827b-5abf-4235-a5b0-b3eea15e9a90",
   "metadata": {},
   "source": [
    "And now check out the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e346c897-95a4-4bd5-9e58-b8fc3e768ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(images)\n",
    "# outputs[0]\n",
    "# classes[np.argmax(outputs[0])]\n",
    "\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join(f'{classes[predicted[j]]}'\n",
    "                              for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b936dc2b-7fb0-4cf5-8f14-d4d8d9550231",
   "metadata": {},
   "source": [
    "We can get the accuracy of across all images in the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbdc12d-1e22-443d-9f81-303eda105e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f5930a-4735-4623-a078-8779fd1149ba",
   "metadata": {},
   "source": [
    "This is much better than random guessing, which would be around 10%. And we trained for about 1 minute, and we were using massively downsized images.\n",
    "\n",
    "## Evaluating a Neural Network\n",
    "\n",
    "To properly evaluate the network, we cannot rely on just the average accuracy above, especially when we have 10 classes or more (later we will see a 1000 class classification problem).\n",
    "\n",
    "Let's see the accuracy by class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf344d2-ba08-4e60-a509-06c2a97b458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "cr_true_labels = []\n",
    "cr_pred_labels = []\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        \n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            cr_true_labels.append(int(label))\n",
    "            cr_pred_labels.append(int(prediction))\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class {classname:5s} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db55701-1f71-4927-8e1f-2d1865657353",
   "metadata": {},
   "source": [
    "We can use the `classification_report()` function from SciKit Learn that we used several times yesterday to print a report. This works even though we did not use SciKit Learn to train the model at all (as it just requires lists of predictions and labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb72b207-bbf0-4cfa-8a23-1812c244f6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(cr_true_labels, cr_pred_labels, target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad0b385-3717-4a8c-a514-1a7f7cb99611",
   "metadata": {},
   "source": [
    "And we can also plot a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b457963-2a1e-4c52-a9af-1ce1cf85d394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(cr_true_labels, cr_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6187f0-3874-4a2c-ad5c-45bd3852acea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "# sns.set(font_scale=1.2)  # Adjust font size\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=classes, \n",
    "            yticklabels=classes)\n",
    "\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('True Labels')\n",
    "plt.yticks(rotation=0)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c595997a-7b84-4dba-81b2-e6bb5d877872",
   "metadata": {},
   "source": [
    "Here you see the confusion across classes of the model's predictions. \n",
    "\n",
    "For example, you can quickly see that cat has a lot of confusion, as it is often confused with dog, or that truck is often confused for car."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df197de-9f34-4b4e-88b6-cc2eb4582939",
   "metadata": {},
   "source": [
    "## Saving a Model\n",
    "\n",
    "Once you have trained a model, you can saved it to be used later. \n",
    "\n",
    "In essence what you are really doing is saving the model's weights. That means that the larger the network, the bigger the saved model. \n",
    "\n",
    "In the case of a network where each weight is some 32-bit precision number (float32), as is the generally the default, then 4 bytes are required per weight. \n",
    "\n",
    "So, for a model with 6 million parameters, we have 4 x 6 million bytes = 24 million bytes, which is 24MB. \n",
    "\n",
    "You can also see how large some of the large language models are. For example Llama from Facebook has 65 billion parameters, this is 260GB of weights for the model. Often, however, you will see 16 bit floats or even less precision, and the files sizes can be smaller, at 1 byte per parameter, which still results in 65GB file size and 65GB of GPU memory required to run the file.\n",
    "\n",
    "Luckily, our trained model is much smaller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86952d9-a2c8-4c04-b6cb-afae32fe556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchinfo.summary(net, (batch_size, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd9a5f5-b114-4981-b892-3905092f2d2f",
   "metadata": {},
   "source": [
    "We have 62,006 parameters, which makes 248,024 bytes, which is only 0.25MB. As you can see there is some overhead, so the entire model is about 2.31 MB. \n",
    "\n",
    "To save the model, we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9650684-0df0-493d-9b4c-ecf09fa35cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), './cifar10-model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd8b898-1708-4570-a0bf-e094205a331b",
   "metadata": {},
   "source": [
    "We can have a look at the size of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ac4dd9-3b2e-4047-891f-f000ba155aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1f7b48-fabc-4095-ab91-892d9ccd863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -lAhF ./cifar10-model.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4ab3a6-2b61-4344-8a16-30fbc4f974c7",
   "metadata": {},
   "source": [
    "A very small model in fact! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ce58f6-eaab-48fd-939b-3168faeb9d7c",
   "metadata": {},
   "source": [
    "To load a model, it is a matter of a few lines of code also:\n",
    "\n",
    "```python\n",
    "model = TheModelClass(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()\n",
    "```\n",
    "\n",
    "We will load a pretrained model in an example later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76f4d93-2050-4dc0-ad51-5988c1688f4c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PyTorch Hub\n",
    "\n",
    "PyTorch Hub lets you download pre-trained models very easily.\n",
    "\n",
    "What is a pretrained model? This is a model that has been trained already, perhaps on millions of images, and where the weights have been made available. \n",
    "\n",
    "Using a pre-trained model saves you the effort of training a model for a long time with data you don't have. \n",
    "\n",
    "These pre-trained models can be used as is, or can be fine-tuned to suit your specific task. We will discuss fine-tuning later.\n",
    "\n",
    "Here we download ResNet18 and ask that we retrieve the pre-trained weights also. Note that is has been trained on ImagetNet, a 1,000 class classification problem. We will look at the classes soon.\n",
    "\n",
    "Download the model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d01098-bf9e-4235-8a19-22fd84238528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models import ResNet18_Weights\n",
    "import torch\n",
    "\n",
    "# Get a ResNet18 pretrained model\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "# Important to set the model to evaluation mode before trying to get predictions. \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b6a5a7-5ce3-4d12-a59b-912169847e92",
   "metadata": {},
   "source": [
    "Once downloaded and loaded, it will print this information.\n",
    "\n",
    "Let's make it predict an image:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efff6602-f97e-4bcd-8fa1-cdb3d7bf897a",
   "metadata": {},
   "source": [
    "First, we download an example image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed30674-4407-48e0-b6bb-82ef7f942dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download an example image from the pytorch website\n",
    "import urllib\n",
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efd7ff9-abc5-4b31-9f51-968ae85c51ee",
   "metadata": {},
   "source": [
    "Then we convert this image in to a format we can view, and preview it here in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a863d73-20db-4df2-a6a0-321f0a74bdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "input_image = Image.open(filename)\n",
    "input_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd88f11-2b1a-43ff-b324-2a1f8cf8927e",
   "metadata": {},
   "source": [
    "Now we can ask the network to make its predictions based on the dog image above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1795620e-2295-4ab5-9b5b-6596d8ec886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    #transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "# move the input and model to GPU for speed if available\n",
    "#if torch.cuda.is_available():\n",
    "#    input_batch = input_batch.to('cuda')\n",
    "#    model.to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "    \n",
    "# Tensor of shape 1000, with confidence scores over ImageNet's 1000 classes\n",
    "# print(output[0])\n",
    "# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2882bff-7e89-416c-82a9-1f6063f8ba70",
   "metadata": {},
   "source": [
    "As you can see, the raw output is a list of 1,000 probabilities. Each probability is a confidence score of the how likely the network thinks the image belongs to that class.\n",
    "\n",
    "These probabilities sum to 1, as we can see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5ad9aa-4012-4025-b9c6-f9d1bb67682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3854df0a-d64d-4b72-8341-36786499ec5b",
   "metadata": {},
   "source": [
    "Let's take the top 5 probabilities which would be the top 5 predictions of which class this image belongs to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80963669-009a-44db-a9c4-4a21ae2a3d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the categories\n",
    "with open(\"./data/imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]\n",
    "\n",
    "# Show top categories per image\n",
    "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "for i in range(top5_prob.size(0)):\n",
    "    print(categories[top5_catid[i]], top5_prob[i].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee3bcaf-10c8-41eb-bbfd-4e9a8c8d79d4",
   "metadata": {},
   "source": [
    "Looks like the network was right, even recognising the difference between a samoyed and a white wolf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66fc468-90dc-4eea-bc9d-7dd37f4d9149",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- We use neural networks for a variety of tasks\n",
    "- Building models from scratch requires quite a lot of in depth knowledge of how neural networks operate\n",
    "- Using pre-built, and pre-trained models we can quickly get started without needing in depth knowledge\n",
    "- You evaluate neural networks in them much the same way as other algorithms such as random forests, however because they are trained over epochs, you also need to watch out for overfitting and watch the network learn over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e9e9e6-16ee-45e7-a8c4-351949a079c2",
   "metadata": {},
   "source": [
    "# Large Language Models\n",
    "\n",
    "In recent years, large language models (LLMs) have gained a lot of attention due to products such as ChatGPT. These are a type of network known as generative models. \n",
    "\n",
    "Generative models also exist in the area of imaging, however we will not cover this. The technlogy behind most of the image generation technology are known as diffusion models, you can read more about this here: <https://en.wikipedia.org/wiki/Diffusion_model>\n",
    "\n",
    "For this we will be using the Transformers package from Hugging Face. This package:\n",
    "\n",
    "> Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to train a model from scratch. These models support common tasks in different modalities, such as:\n",
    ">\n",
    "> - 📝 Natural Language Processing: text classification, named entity recognition, question answering, language modeling, code generation, summarization, translation, multiple choice, and text generation.\n",
    "> - 🖼️ Computer Vision: image classification, object detection, and segmentation.\n",
    "> - 🗣️ Audio: automatic speech recognition and audio classification.\n",
    "> - 🐙 Multimodal: table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.\n",
    ">\n",
    "> *Source: <https://huggingface.co/docs/transformers/en/index>*\n",
    "\n",
    "If we visit the link above, we will see that there are literally hundreds of models that you can download.\n",
    "\n",
    "Most of these models can be fine-tuned to your needs, and come pre-trained.\n",
    "\n",
    "LLMs are very versitile and can be used for a number of tasks. For example, they can be used to summarise text, or for sentiment analysis.\n",
    "\n",
    "LLMs use the concept of tokenisers to split your text in to tokens. These can more or less be considered words. \n",
    "\n",
    "LLMs are trained on really large datasets of text, where the job of the LLM is to predict the next token given some text. \n",
    "\n",
    "Networks are trained using sequences of text, which are gathered from books, newspapers, journal articles, and so on (know as a corpus). Within this dataset, we have millions, if not billions of sentences. For example, let's say that this sentence appears in our corpus:\n",
    "\n",
    "```\n",
    "The weather is very nice today\n",
    "```\n",
    "\n",
    "We can take a subset of that text, and leave out the last word. Meaning we know the next token in the sentence (`today`).\n",
    "\n",
    "So, we take the following:\n",
    "\n",
    "```\n",
    "The weather is very nice \n",
    "```\n",
    "\n",
    "And we ask network to predict the next word. We know the correct answer is `today`. Therefore if the network returns:\n",
    "\n",
    "```\n",
    "The weather is very nice yesterday\n",
    "```\n",
    "\n",
    "We can then tell the network this was incorrect, and update the network's weights accorindly. \n",
    "\n",
    "If however, the network predicted the following:\n",
    "\n",
    "```\n",
    "The weather is very nice today\n",
    "```\n",
    "\n",
    "then we know this is correct, as we know that `today` is the correct answer. We can therefore update the network's weights accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a9c654-ac0f-48bb-a38b-ac3bf05112ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ed5baa-f4a6-4fb6-8dce-b8d4764c101f",
   "metadata": {},
   "source": [
    "Now we will load the tokeniser and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc9ae35-8549-4413-a7d9-6ae1a70d6300",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb11d569-f279-4cc1-b87c-ebc8a76fe418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate tokenization\n",
    "text = \"Welcome to the ATSP 2025 course!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "token_ids = tokenizer.encode(text)\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0b47f2-f4dc-4d45-8aae-ad21fe65bbfd",
   "metadata": {},
   "source": [
    "We can go from tokenised text back to original text in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b964c1ea-042d-41f9-a067-7e8f943630a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show token-to-text conversion\n",
    "decoded = tokenizer.decode(token_ids)\n",
    "print(\"Decoded text:\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7dbebb-d30f-42b9-acee-3de5a378a7a0",
   "metadata": {},
   "source": [
    "Why is tokenisation performed? Basically it is so that words, which are variable in length, can be treated as fixed length vectors. This makes it much easier to train a neural network, as every word that is input is a vector of length 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e19c3db-086e-49cb-9648-0d8f88a83009",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8af5937-7b8c-4150-8597-5b490674607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sentiment analysis pipeline\n",
    "sentiment_analyzer = pipeline('sentiment-analysis', model='distilbert/distilbert-base-uncased-finetuned-sst-2-english')\n",
    "\n",
    "# Interactive examples\n",
    "texts = [\n",
    "    \"I really like the ATSP 2025 class a lot.\",\n",
    "    \"I'm not sure I understand the concepts Marcus is trying to teach me.\",\n",
    "    \"Learning about LLMs and AI is exciting! But I am not sure how well it will help me in my day to day life.\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    result = sentiment_analyzer(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {result[0]['label']}\")\n",
    "    print(f\"Confidence: {result[0]['score']:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad70321-8e09-46d1-987c-88fabb92cf99",
   "metadata": {},
   "source": [
    "Sentiment analysis is not just limited to positive or negative, it could also be used for some kind of patient wellbeing analysis or anything along those lines. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a1822d-0230-463e-af93-b8908d36ed89",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "What is particularly used in medicine is a sub-category of nautrual language processing called Named Entity Recognition. \n",
    "\n",
    "From the Wikipedia article:\n",
    "\n",
    "> Named-entity recognition is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.\n",
    "\n",
    "For example, it might be useful to extract ICD codes from medical texts using such a technique. Often, such extraction techniques use rule-based approaches that are quite prone to failure.\n",
    "\n",
    "Let's try this out using Transformers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7d8e5d-82a0-4d34-97c3-2d9d064c1011",
   "metadata": {},
   "source": [
    "Example text:\n",
    "\n",
    "> Mrs. Finley presents today after having a new cabinet fall on her last week (W20.8XXA), suffering a concussion, as well as some cervicalgia. She was cooking dinner (Y93.G3) at the home she shares with her husband. She did not seek treatment at that time. She states that the people that put in the cabinet in her kitchen (Y92.010) missed the stud by about two inches. Her husband, who was home with her at the time told her she was 'out cold' for about two minutes (S06.0X1A). The patient continues to have cephalgias since it happened, primarily occipital, extending up into the bilateral occipital and parietal regions. The headaches come on suddenly, last for long periods of time, and occur every day. They are not relieved by Advil (G44.311). She denies any vision changes, any taste changes, any smell changes. The patient has a marked amount of tenderness across the superior trapezius.\n",
    "> \n",
    "> *Source: https://www.aapc.com/icd-10/icd-10-documentation-example.aspx*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f40fdc-585b-4fab-b3de-4e3e0fbdfa32",
   "metadata": {},
   "source": [
    "Once extracted, we can look at specific tokens that it found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf843ce6-89fb-42af-918f-91d608034098",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = AutoTokenizer.from_pretrained(\"ugaray96/biobert_ncbi_disease_ner\")\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"ugaray96/biobert_ncbi_disease_ner\")\n",
    "\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokeniser)\n",
    "\n",
    "# Source: https://huggingface.co/ugaray96/biobert_ncbi_disease_ner\n",
    "text = \"\"\"The patient was diagnosed with lung cancer and started \n",
    "chemotherapy. He also haas a history of diabetes and heart disease.\"\"\"\n",
    "\n",
    "# Source: https://www.aapc.com/icd-10/icd-10-documentation-example.aspx\n",
    "long_text = \"\"\"\n",
    "Mrs. Finley presents today after having a new cabinet fall on her last week \n",
    "(W20.8XXA), suffering a concussion, as well as some cervicalgia. She was \n",
    "cooking dinner (Y93.G3) at the home she shares with her husband. She \n",
    "did not seek treatment at that time. She states that the people that \n",
    "put in the cabinet in her kitchen (Y92.010) missed the stud by about \n",
    "two inches. Her husband, who was home with her at the time told her \n",
    "she was out cold for about two minutes (S06.0X1A). The patient continues \n",
    "to have cephalgias since it happened, primarily occipital, extending \n",
    "up into the bilateral occipital and parietal regions. The headaches \n",
    "come on suddenly, last for long periods of time, and occur every day. \n",
    "They are not relieved by Advil (G44.311). She denies any vision changes, \n",
    "any taste changes, any smell changes. The patient has a marked amount \n",
    "of tenderness across the superior trapezius.\n",
    "\"\"\"\n",
    "\n",
    "result = ner_pipeline(long_text)\n",
    "\n",
    "diseases = []\n",
    "\n",
    "for entity in result:\n",
    "    if entity[\"entity\"] == \"Disease\":\n",
    "        word = entity['word']\n",
    "        word = word.strip()\n",
    "        word = entity['word'][2:] if entity['word'].startswith('##') else entity['word']\n",
    "        diseases.append(f\"{word} \")\n",
    "    elif entity[\"entity\"] == \"Disease Continuation\" and diseases:\n",
    "        word = entity['word']\n",
    "        word = word.strip()\n",
    "        word = entity['word'][2:] if entity['word'].startswith('##') else entity['word']\n",
    "        diseases[-1] += f\"{word}\"\n",
    "\n",
    "print(f\"Diseases: {', '.join(diseases)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b157b6-0f08-4e46-bdb6-47d39215fdad",
   "metadata": {},
   "source": [
    "## Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255d677f-a56d-4b36-a0cc-2e1ed6fb1639",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pipeline = pipeline(\"question-answering\", model='distilbert/distilbert-base-cased-distilled-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a375f31a-b2e5-4419-860f-050c72c20d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"Mrs. Finley presents today after having a new cabinet \n",
    "fall on her last week (W20.8XXA), suffering a concussion, as \n",
    "well as some cervicalgia. She was cooking dinner (Y93.G3) at \n",
    "the home she shares with her husband. She did not seek treatment \n",
    "at that time. She states that the people that put in the cabinet \n",
    "in her kitchen (Y92.010) missed the stud by about two inches. \n",
    "Her husband, who was home with her at the time told her she \n",
    "was out cold for about two minutes (S06.0X1A). The patient \n",
    "continues to have cephalgias since it happened, primarily \n",
    "occipital, extending up into the bilateral occipital and parietal \n",
    "regions. The headaches come on suddenly, last for long periods of \n",
    "time, and occur every day. They are not relieved by Advil (G44.311). \n",
    "She denies any vision changes, any taste changes, any smell changes. \n",
    "The patient has a marked amount of tenderness across the superior trapezius.\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"What happened to Mrs. Finley?\",\n",
    "    \"What did her husband do?\",\n",
    "    \"Has she taken medication?\",\n",
    "    \"What are the ICD codes from this text?\"\n",
    "]\n",
    "# Get answers to the questions\n",
    "for question in questions:\n",
    "    answer = qa_pipeline(question=question, context=context)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer['answer']}\")\n",
    "    print(f\"Confidence: {answer['score']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc8f8f3-133a-4865-8d06-26073ba7ce3f",
   "metadata": {},
   "source": [
    "### Text Generation\n",
    "\n",
    "We can of course use LLMs to generate text also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bfb265-ac8a-4614-8ce4-16e1c5152a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 model and tokenizer\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "generator.generation_config.pad_token_id = tokenizer.pad_token_id  # Stop warnings about tokeniser padding\n",
    "\n",
    "# Generate text from prompts\n",
    "prompts = [\n",
    "    \"Artificial Intelligence is\",\n",
    "    \"The future of technology will\",\n",
    "    \"Students learning about AI should\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    generated = generator(prompt, max_length=50, num_return_sequences=1, truncation=True)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {generated[0]['generated_text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f837f658-1fd6-4021-9b8d-f80060348bbd",
   "metadata": {},
   "source": [
    "## Translation\n",
    "\n",
    "We can even use it to translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7782f068-e1ce-4b42-91de-f9cc03199f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_to_de = pipeline('translation_en_to_de', model='google-t5/t5-base')\n",
    "\n",
    "german_result = en_to_de(\"\"\"The patient \n",
    "continues to have cephalgias since it happened, primarily \n",
    "occipital, extending up into the bilateral occipital and parietal \n",
    "regions. The headaches come on suddenly, last for long periods of \n",
    "time, and occur every day. They are not relieved by Advil\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60c8f44-9bd0-488a-a291-496de433202d",
   "metadata": {},
   "source": [
    "Once this is complete, we can view the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f091f42-bb24-453d-a9c4-ce61e3923f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(german_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969b6737-5f3c-4699-bf96-b90c7059eee1",
   "metadata": {},
   "source": [
    "## Text Summarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd3b09f-72cd-4754-abbb-bd98a1a711fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarise = pipeline(\"summarization\", model='sshleifer/distilbart-cnn-12-6')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e209f191-54ce-490c-8b28-f33dc8ab46a4",
   "metadata": {},
   "source": [
    "Now that this has loaded, we can summarise text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1cdf35-d625-45c7-a4bf-2483f744b7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_text = \"\"\"Mrs. Finley presents today after having a new cabinet \n",
    "fall on her last week, suffering a concussion, as \n",
    "well as some cervicalgia. She was cooking dinner at \n",
    "the home she shares with her husband. She did not seek treatment \n",
    "at that time. She states that the people that put in the cabinet \n",
    "in her kitchen missed the stud by about two inches. \n",
    "Her husband, who was home with her at the time told her she \n",
    "was out cold for about two minutes. The patient \n",
    "continues to have cephalgias since it happened, primarily \n",
    "occipital, extending up into the bilateral occipital and parietal \n",
    "regions. The headaches come on suddenly, last for long periods of \n",
    "time, and occur every day. They are not relieved by Advil. \n",
    "She denies any vision changes, any taste changes, any smell changes. \n",
    "The patient has a marked amount of tenderness across the superior trapezius.\"\"\"\n",
    "\n",
    "summary_short = summarise(long_text, max_length=75, min_length=30, do_sample=False)\n",
    "summary_medium = summarise(long_text, max_length=150, min_length=50, do_sample=False)\n",
    "\n",
    "print(\"Text Summarization Pipeline Results:\")\n",
    "print(\"Original text length:\", len(long_text.split()))\n",
    "print(\"\\nShort Summary:\")\n",
    "print(summary_short[0]['summary_text'])\n",
    "\n",
    "print(\"\\nMedium Summary:\")\n",
    "print(summary_medium[0]['summary_text'])\n",
    "\n",
    "print('\\n' + '-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdbcb52-f7e3-477c-94c0-0c966a7b9b1f",
   "metadata": {},
   "source": [
    "And we could translate the summary in to German:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c180c90-76c2-4b25-9c20-59fd90d14cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_to_de(summary_short[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ab3927-ddee-4758-b506-8b98b1110e6f",
   "metadata": {},
   "source": [
    "# Medical Image Segmentation\n",
    "\n",
    "In this section we will discuss the area of medicine where deep learning has probably the largest impact, that is in image segmentation. \n",
    "\n",
    "![segmentation](./img/segmentation-numbered.png)\n",
    "\n",
    "*Source:* University of Waterloo Faculty of Mathematics, <https://wiki.math.uwaterloo.ca/statwiki/index.php?title=Mask_RCNN>\n",
    "\n",
    "As you can see from the image above, there are several types of problems that can be addressed using deep learning. We have seen image classification above, but there is also 1) Image Recognition, where several objects can be detected in an image (for example lung nodules), 2) Object Detection, where bounding boxes are placed around the objects, 3) Semantic Segmentation, where different classes of object are detected on a pixel by pixel basis, and 4) Instance Segmentation where classes and instances of classes are detected. \n",
    "\n",
    "In medical imaging, currently by far the most research effort is spent on tasks 3 and 4, Semantic Segmentation and Instance Segmentation, and in particular 3D image segmentation, such as CT or MR imaging in the area of Radiology. Of course, 2D segmentation is also a topic of interest, for example Whole Slide image analysis in the field of Pathology. \n",
    "\n",
    "In this section we will look at some of ways in which you could train such a network, and what kind of data is required to do so. \n",
    "\n",
    "We will not train such a model live, as 3D segmentation models tend to require days or even weeks of training time.\n",
    "\n",
    "## Foundation Models\n",
    "\n",
    "Meta recently released SAM, the Segment Anything Model. This is a model known as a 'foundation model' and has been trained on 10s of millions of images, from many different areas. Therefore, these are general segmentation models, and can perform segmentation on a wide variety of different \n",
    "\n",
    "This contrasts to the very specific networks that you might train for \n",
    "\n",
    "We can take a look at the SAM demo, and use it to segment a brain MRI.\n",
    "\n",
    "https://sam2.metademolab.com/\n",
    "\n",
    "Note that these foundation models, however, require some form of user input to work. They are not completely automated. For this, a specific model for a specific tumour type, for example, needs to be trained. \n",
    "\n",
    "## U-Nets\n",
    "\n",
    "U-Nets are a special type of deep network that are used specifically for the task of learning. We will not discuss them in detail, only to mention that they are the current state of the art at image segmentation. \n",
    "\n",
    "If you wish to use a U-Net to perform segmentation, then by far the most straighfoward way to do this is using nnUNet, a framework for automatically configuring U-Nets: See: <https://github.com/MIC-DKFZ/nnUNet>\n",
    "\n",
    "## Data \n",
    "\n",
    "The biggest hurdle in developing segmentation models is getting **annotated** data. If you think about MR, like the brain MR we saw previously, each layer of the MR needs to be carefully annotated with segmentations by a radiologist. This is manual work and requires expertise and experience. For rare diseases, there may not be that many radiologists who are qualified to perform the annotations. Hence, getting enough data to train these deep segmentation models can be a challenging task.\n",
    "\n",
    "There are a number of resources for finding segmentation \n",
    "\n",
    "### The Cancer Imaging Archive\n",
    "\n",
    "The Cancer Imaging Archive is a repository of image data that is freely and openly available. \n",
    "\n",
    "As an example, here is a dataset regarding Soft Tissue Sarcoma: <https://www.cancerimagingarchive.net/collection/soft-tissue-sarcoma/>. This is a collection of MR and CT images of various sarcoma, including liposarcoma and fibrosarcoma. There are a total of 51 patients in the dataset, and the MR images have been annotated by radiologists, meaning the tumours have been highlighted and annotated manually for every layer of each MR. \n",
    "\n",
    "In order to navigate it, do the following:\n",
    "\n",
    "- URL: https://www.cancerimagingarchive.net\n",
    "- Navigate to \"Access the Data\" -> \"Data Portals Dashboard\"\n",
    "- From here, you can access Radiology Portal or the Pathology Portal, as well as the Browse Collections feature\n",
    "\n",
    "If you open the Radiology portal for example, you will eventually ger here: <https://nbia.cancerimagingarchive.net/nbia-search/> \n",
    "\n",
    "The browser lets you filter by modaility, such as MR, anatomical site, such as prostate or pancreas, and by study. Studies are collections of images that might come from different anatomical sites and institutions, but are grouped due to being part of some study. Once you have narrowed down your search, click the \"Search Results\" tab in order to see the data.\n",
    "\n",
    "To download data, you need to use the NBIA (National Biomedical Imaging Archive) Data Retriever. When you browse TCIA, you can add data to a cart, and once you are finished, you use the NBIA Data Retriever to download all the data in bulk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab3dc2d-a7cd-4498-8ebc-bf15b1a84497",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Medical Image Analysis: An Example\n",
    "\n",
    "In the following paper by Beck, et al. we see the following:\n",
    "\n",
    "![Patho](./img/patho-workflow.png)\n",
    "\n",
    "*Source*: Beck, A.H., Sangoi, A.R., Leung, S., Marinelli, R.J., Nielsen, T.O., Van De Vijver, M.J., West, R.B., Van De Rijn, M. and Koller, D. Systematic analysis of breast cancer morphology uncovers stromal features associated with survival. Science Translational Medicine, 3(108), 2011. <https://www.science.org/doi/10.1126/scitranslmed.3002564>\n",
    "\n",
    "This highlights exactly the difference between the classical approache that we looked at yesterday, and the deep learning approach. \n",
    "\n",
    "The workflow from the paper above is time consuming, requires much more expertise and effort, as well as a lot of domain knowledge.\n",
    "\n",
    "The deep networks, you could likely skip the vast majority of that work, and simply train a network to do this from end to end, without wortying about any of these intermediate steps. It would require none of the preprocessing and expertise and domain knowledge that was required to do the work in the paper above.\n",
    "\n",
    "This is a quite recent development, the paper above is from 2011 and certainly before deep learning became mainstreeam. \n",
    "\n",
    "We can illustrate this now by training a network to predict skin lesions types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d0a07e-376a-4d03-8413-5a9c1402b3bf",
   "metadata": {},
   "source": [
    "# MedMNIST\n",
    "\n",
    "We have seen MNIST, let's take a look at MedMNIST.\n",
    "\n",
    "MedMNIST is a collection of pre-processed medical image datasets, designed specifically for benchmarking machine learning models in the context of healthcare. To demonstrate its use, we will walk you through the process of loading some of its datasets and previewing the data using Python.\n",
    "\n",
    "Let's make a number of imports and define a function to view the images easily, as we will do this several times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c678070-7d0f-4eb6-b97e-0232dfbd62a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import medmnist\n",
    "from medmnist import Evaluator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Let's also define a function to visualise images easily later\n",
    "def visualise_medmnist(data, num_images=6):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(2, 3, i + 1)\n",
    "        idx = random.randint(0, len(data) - 1)\n",
    "        image, label = data[idx]\n",
    "        # image = image.squeeze()  # Remove channel dimension if it exists\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title(f'Label: {label}')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708860db-2fa6-4c20-a75f-a18d13c3b921",
   "metadata": {},
   "source": [
    "There are several datasets included in MedMNIST (see <https://medmnist.com>), covering a wide arary of medical image types:\n",
    "\n",
    "![MedMNIST](./img/MedMNIST.png)\n",
    "\n",
    "*Source*: Yang J, Shi R, Wei D, Liu Z, Zhao L, Ke B, Pfister H, Ni B. MedMNIST v2-A large-scale lightweight benchmark for 2D and 3D biomedical image classification. Nature Scientific Data. 2023 Jan 19;10(1):41.\n",
    "\n",
    "The data included in MedMNIST covers several types of medical imagery, from dermatology, to histopathology as well as 3D datasets such as the OrganMNIST3D abdominal CT dataset. \n",
    "\n",
    "Let's have a look at the Dermatology dataset.\n",
    "\n",
    "The data can be downloaded in various file sizes, which you can specify with the `size=128` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbe1af3-adab-46ab-af4e-4befcc47b11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medmnist import DermaMNIST\n",
    "\n",
    "derma_dataset = DermaMNIST(split=\"train\", download=True, size=128)  # Can be up to 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b556cd-8001-4391-9871-665a06a978cf",
   "metadata": {},
   "source": [
    "The documentation for MedMNIST is actaully not very good on the official website, however it is well documented within the package itself. \n",
    "\n",
    "Therefore, we can use `?` at any time, as we have seen several times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b0ecb5-29ca-4406-8f03-ddacbe1a94ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "DermaMNIST?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e13572a-a959-47f4-9acf-50e8f6f5343d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's preview the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999bcc5a-eb0f-4429-a26b-58b317a2d457",
   "metadata": {},
   "outputs": [],
   "source": [
    "derma_dataset.montage(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42dab08-b3bb-467c-8e81-18704629a871",
   "metadata": {},
   "source": [
    "Using the `visualise_medmnist` function we defined above, we can preview this data. The function prints a random 6 images from the dataset. Each time we call the function we will get another random 6 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfddf39-450c-4ddc-82e1-44e95afe6037",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_medmnist(derma_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfb845c-f18f-499f-989b-79154252f8c0",
   "metadata": {},
   "source": [
    "First thing to notice is that we are dealing with a dataset of skin lesions that were collected using a **dermatoscope**.\n",
    "\n",
    "A dermatoscope is similar to a camera, and provides a non-invasive method to analyse skin lesions:\n",
    "\n",
    "![Dermatoscope](./img/dermatoscope.jpeg)\n",
    "\n",
    "*Source*: <https://commons.wikimedia.org/w/index.php?curid=2431174> \n",
    "\n",
    "The camera can fully enclose the lesion and therefore is unaffected by ambient light, and has its own powerful light that illuminates the lesion in a way that enhances the visibility of surface patterns and colours not seen by the naked eye. Dermatoscopes have a magnifier, typically around 10x. \n",
    "\n",
    "This close-up view provides detailed information about the morphology, skin lesions, melanomas and other types of skin cancer, moles and so on.\n",
    "\n",
    "Let's take a look at what `derma_dataset` contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddea2dce-f675-43bc-bacd-7c26ce4e6452",
   "metadata": {},
   "outputs": [],
   "source": [
    "derma_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee90e3cd-83d6-4002-be41-3c882060b4a8",
   "metadata": {},
   "source": [
    "By reading the text, we can see, we have 10,015 images in total. They are already split in to a train, validation, and test sets as this is a benchmarking dataset (therefore various groups might want to compare their results using the exact same test set).\n",
    "\n",
    "So the dataset has a total of 10,015 images and they are split up as follows:\n",
    "\n",
    "| Split      | Number     |\n",
    "|------------|------------|\n",
    "| Train      | 7,007      |\n",
    "| Validation | 1,003      |\n",
    "| Test       | 2,005      |\n",
    "| **Total**  | **10,015** |\n",
    "\n",
    "The images are in colour, as indicated by the `Number of channels: 3` for the three channels of RGB.\n",
    "\n",
    "Last, the images are of 7 distinct diseases, and is therefore a multi-class classification problem.\n",
    "\n",
    "The 7 diseases are shown in the `Meaning of labels` field and are as follows: \n",
    "\n",
    "0. actinic keratoses and intraepithelial carcinoma\n",
    "1. basal cell carcinoma\n",
    "2. benign keratosis-like lesions\n",
    "3. dermatofibroma\n",
    "4. melanoma\n",
    "5. melanocytic nevi\n",
    "6. vascular lesions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b09b7f-6ba3-4d84-a5a5-fabca6a99b14",
   "metadata": {},
   "source": [
    "Maybe it would be interesting to see how each of the 7 different classes look. For example, are the melanoma vastly different in appearance to nevi? \n",
    "\n",
    "Note here that melanocytic nevi or nevi are benign lesions normally called moles, while melanoma are cancerous, malignant lesions that are harmful and can spread (metastasise). \n",
    "\n",
    "We can also take a look at how each class looks.\n",
    "\n",
    "We can use `np.where()` to search for subsets of data by their labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71c7af1-9d79-431d-9647-662ccd90577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['keratoses', 'basal', 'benign keratosis', 'dermatofibroma', 'melanoma', 'nevi', 'vascular']\n",
    "\n",
    "keratoses_idx = np.where(derma_dataset.labels==0)[0]\n",
    "basal_idx = np.where(derma_dataset.labels==1)[0]\n",
    "benign_keratoses_idx = np.where(derma_dataset.labels==2)[0]\n",
    "dermatofibroma_idx = np.where(derma_dataset.labels==3)[0]\n",
    "melanoma_idx = np.where(derma_dataset.labels==4)[0]\n",
    "nevi_idx = np.where(derma_dataset.labels==5)[0]\n",
    "vascular_idx = np.where(derma_dataset.labels==6)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad0d926-f5c4-4017-bd4f-86fcde9e0258",
   "metadata": {},
   "source": [
    "Now we can preview each class to see if something interesting is going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be6ff7f-b170-44ab-976c-37a191123878",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_medmnist([(derma_dataset.imgs[i], \"Keratosis\") for i in keratoses_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e3d885-65fb-4e40-80d0-dccbf2437340",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_medmnist([(derma_dataset.imgs[i], \"Melanoma\") for i in melanoma_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7113707c-799e-4390-9e8b-31c95f5492f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_medmnist([(derma_dataset.imgs[i], \"Nevi\") for i in nevi_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e24200-0b98-4e4f-b0fb-1cdd7adf9375",
   "metadata": {},
   "source": [
    "## ABCDE\n",
    "\n",
    "If we think back to how we might tackle the problem of classifying or diagnosing skin lesions based on features, it might make sense to understand how a dermatologist or doctor might make such a diagnosis. \n",
    "\n",
    "Upon visual examination by dermatologist, the following characteristics are normally noted (ABCDE test):\n",
    "\n",
    "- A: Asymmetry - melanoma often present as asymetrical and non-uniform, non-round. \n",
    "- B: Border - melanoma have less well-defined borders than non-cancerous moles. They appear to smudge between skin tissue and the lesion itself.\n",
    "- C: Colour - melanoma are darker in appearance (melanoma start in the melanocytes, and these are the cells that give your skin its pigment) and have multiple shades of colours, while non-cancerous moles tend to have one colour\n",
    "- D: Diameter - melanoma are larger and will grow larger than 5 or 6mm. \n",
    "- E: Evolution - melanoma tend to change shape, size, and colour oven time, or nayb begin to itch or bleed, unlike benign moles.\n",
    "\n",
    "See: <https://www.mayoclinic.org/diseases-conditions/melanoma/symptoms-causes/syc-20374884>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e596d96-6a65-4c8e-af65-2cfd184861a2",
   "metadata": {},
   "source": [
    "## Closer Look at the Data\n",
    "\n",
    "Let's take a closer look at the data and determine how to train a network with it.\n",
    "\n",
    "For example, we can take a look at the distribution of the various classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79366906-824e-415d-8eee-589f7c53cdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get the unique values and their counts/frequencies\n",
    "unique, counts = np.unique(derma_dataset.labels, return_counts=True)\n",
    "\n",
    "# Print nicely using a DataFrame\n",
    "pd.DataFrame(np.asarray((unique, counts)).T, columns=['class id', 'freq.'], index=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6ee2f2-4dc6-415c-baa6-3c4d90857cf3",
   "metadata": {},
   "source": [
    "Notice the very large imbalance!\n",
    "\n",
    "We will see how this might be an issue later.\n",
    "\n",
    "First though, let's train a network.\n",
    "\n",
    "First, define some values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6aa2d3-78b4-4137-a1ce-f034737767fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_epochs = 3\n",
    "batch_size = 128\n",
    "lr = 0.001\n",
    "\n",
    "# task = 'multi-task'\n",
    "number_of_channels = 3\n",
    "number_of_classes = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cdc60e-332a-42d8-9fce-5c699bafdee5",
   "metadata": {},
   "source": [
    "Load the relevant Torch libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df24cfaa-6ed0-4e46-be4f-7671343906af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "# from torchvision.transforms import v2 as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67493fd-586c-4136-ad2a-6f04b82270b5",
   "metadata": {},
   "source": [
    "Apply transforms, including resizing the images to 28x28:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70680ed-5d01-41f9-981d-4c0189c0d97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomResizedCrop(size=(28, 28), antialias=True),\n",
    "    #transforms.RandomHorizontalFlip(p=0.5),\n",
    "    #transforms.RandomInvert(p=0.5),\n",
    "    # transforms.Normalize(mean=[.5], std=[.5])\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43ccc72-a322-4676-8527-73c1c9712965",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DermaMNIST(split='train', transform=data_transform, download=True)\n",
    "test_dataset = DermaMNIST(split='test', transform=data_transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fa76ad-d27c-4250-ab82-66d3a09542a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encapsulate data into dataloader form\n",
    "train_loader = data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "train_loader_at_eval = data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = data.DataLoader(dataset=test_dataset, batch_size=2*batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81a7a8d-4fc3-44c4-bd9a-7c0e646411b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple CNN model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 4 * 4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = Net(in_channels=number_of_channels, num_classes=number_of_classes)\n",
    "    \n",
    "# define loss function and optimizer\n",
    "#if task == \"multi-label, binary-class\":\n",
    "#    criterion = nn.BCEWithLogitsLoss()\n",
    "#else:\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b226bdd1-a7a3-4020-b08a-de33d4f90f04",
   "metadata": {},
   "source": [
    "We can check out the model's structure here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da93faa-4211-4e78-8671-9600bc2ae2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchinfo.summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb128e9-7153-4189-b4f1-8a8f14527535",
   "metadata": {},
   "source": [
    "And now do the actual training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7e1953-3fbe-4822-bdac-69d261479b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    model.train()\n",
    "    for inputs, targets in tqdm(train_loader):\n",
    "        # forward + backward + optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        #if task == 'multi-label, binary-class':\n",
    "        #    targets = targets.to(torch.float32)\n",
    "        #    loss = criterion(outputs, targets)\n",
    "        #else:\n",
    "        targets = targets.squeeze().long()\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c49015-75a6-45cb-a83c-86dd94b6e34f",
   "metadata": {},
   "source": [
    "Evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39be09f5-a88a-4c1f-8975-5fe8cf49bb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred_cr = []\n",
    "y_true_cr = []\n",
    "\n",
    "def test(split):\n",
    "    model.eval()\n",
    "    y_true = torch.tensor([])\n",
    "    y_score = torch.tensor([])\n",
    "    \n",
    "    data_loader = train_loader if split == 'train' else test_loader\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            #if task == 'multi-label, binary-class':\n",
    "            #    targets = targets.to(torch.float32)\n",
    "            #    outputs = outputs.softmax(dim=-1)\n",
    "            #else:\n",
    "            targets = targets.squeeze().long()\n",
    "            outputs = outputs.softmax(dim=-1)\n",
    "            targets = targets.float().resize_(len(targets), 1)\n",
    "\n",
    "            y_true = torch.cat((y_true, targets), 0)\n",
    "            y_score = torch.cat((y_score, outputs), 0)\n",
    "\n",
    "        \n",
    "        y_true = y_true.numpy()\n",
    "        y_score = y_score.detach().numpy()\n",
    "        \n",
    "        evaluator = Evaluator('dermamnist', split)\n",
    "        metrics = evaluator.evaluate(y_score)\n",
    "    \n",
    "        # print('%s  auc: %.3f  acc:%.3f' % (split, *metrics))\n",
    "        print(f\"Model accuracy: {metrics[1]:.3f}\")\n",
    "        \n",
    "        # For classification report\n",
    "        for score in y_score:\n",
    "            y_pred_cr.append(np.argmax(score))\n",
    "\n",
    "        for score in y_true:\n",
    "            y_true_cr.append(int(score[0]))\n",
    "        \n",
    "#test('train')\n",
    "test('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd93b27b-c0ff-424b-b137-556e01ef052a",
   "metadata": {},
   "source": [
    "At first, this looks pretty good, close to 70% accuracy. \n",
    "\n",
    "Let's break down the numbers. \n",
    "\n",
    "First we print a classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2255eec7-cbb4-47e2-b8b1-92968d015e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true_cr, y_pred_cr, zero_division=0.0, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f693fbb1-7958-458f-b86a-3ff44eb2600a",
   "metadata": {},
   "source": [
    "Let's try a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79942853-2647-4956-b570-e5f909827b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true_cr, y_pred_cr)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.set(font_scale=1.2)  # Adjust font size\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=labels, \n",
    "            yticklabels=labels)\n",
    "\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix for Iris Dataset (SVM)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcf95e6-94c3-48a6-8f0a-2e71003fa1ac",
   "metadata": {},
   "source": [
    "## Exercise \n",
    "\n",
    "### Question 1\n",
    "\n",
    "We got an average accuracy of nearly 70%.\n",
    "\n",
    "Is this a good result?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb31f808-1250-4c91-9a8e-7734caa9feb2",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9984014e-2a32-4035-be6b-850dd1115110",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "How can the average accuracy look high even if the network is not performing well at all?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9338f3d-61b1-4976-8540-5eb6b96d20dc",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd03e00f-c156-4e05-a1f0-316343e0619b",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Interpret the confusion matrix and the classification report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b974a092-cce6-4381-9691-d2fb31aaae52",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac15124-6138-46cf-874a-5f0c7982c0f1",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Would it be possible to restructure the test set to better evaluate model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d72aa35-e90c-442b-bb1b-8d918e0f33df",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84145ac2-109b-4f71-a783-7193365f3438",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "What might have caused the model to classify all lesions as one class (nevi)? For example, does the dataset contain some kind of class imbalance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99345885-f60b-4bd4-aab5-c77f1d91b867",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55116a35-0a51-4059-a3ab-8b78ca4095ac",
   "metadata": {},
   "source": [
    "## End of Session\n",
    "\n",
    "In the next session we will discuss the following:\n",
    "\n",
    "- How to fine-tune pre-trained networks\n",
    "- How to get more data\n",
    "- How to save models\n",
    "- How to publish and distribute models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
